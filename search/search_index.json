{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Extract Emails","text":""},{"location":"#index","title":"Index","text":"<ul> <li>Quick Start</li> <li>Code References</li> </ul>"},{"location":"changelogs/v5/","title":"V5","text":""},{"location":"changelogs/v5/#530","title":"5.3.0","text":""},{"location":"changelogs/v5/#changed","title":"Changed","text":"<ul> <li>Add custom save mode to csv data saver</li> </ul>"},{"location":"changelogs/v5/#520","title":"5.2.0","text":""},{"location":"changelogs/v5/#added","title":"Added","text":"<ul> <li>CLI tool</li> <li>csv data saver</li> </ul>"},{"location":"changelogs/v5/#513","title":"5.1.3","text":""},{"location":"changelogs/v5/#changed_1","title":"Changed","text":"<ul> <li>Update dependencies</li> </ul>"},{"location":"changelogs/v5/#512","title":"5.1.2","text":""},{"location":"changelogs/v5/#added_1","title":"Added","text":"<ul> <li>Python 3.10 support</li> <li>Add CHANGELOG.md</li> </ul>"},{"location":"changelogs/v5/#510","title":"5.1.0","text":""},{"location":"changelogs/v5/#added_2","title":"Added","text":"<ul> <li>Add save_as_csv class method to <code>PageData</code> model</li> <li>Add logs to DefaultWorker</li> </ul>"},{"location":"changelogs/v5/#changed_2","title":"Changed","text":"<ul> <li>Check if needed libraries for browsers were installed. If not will show user-friendly error</li> <li>Small improvements in the code</li> </ul>"},{"location":"changelogs/v5/#502","title":"5.0.2","text":""},{"location":"changelogs/v5/#changed_3","title":"Changed","text":"<ul> <li>Fix imports for factories and DefaultWorker</li> </ul>"},{"location":"code/browsers/","title":"Browsers","text":""},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter","title":"<code>PageSourceGetter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>All browsers must inherit from this class</p> Source code in <code>extract_emails/browsers/page_source_getter.py</code> <pre><code>class PageSourceGetter(ABC):\n    \"\"\"All browsers must inherit from this class\"\"\"\n\n    def __enter__(self) -&gt; PageSourceGetter:\n        \"\"\"Context manager enter method.\n\n        Returns:\n            Self instance for method chaining\n        \"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"Context manager exit method.\n\n        Args:\n            exc_type: Exception type\n            exc_val: Exception value\n            exc_tb: Exception traceback\n        \"\"\"\n        self.stop()\n\n    async def __aenter__(self) -&gt; PageSourceGetter:\n        \"\"\"Async context manager enter method.\n\n        Returns:\n            Self instance for method chaining\n        \"\"\"\n        await self.astart()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"Async context manager exit method.\n\n        Args:\n            exc_type: Exception type\n            exc_val: Exception value\n            exc_tb: Exception traceback\n        \"\"\"\n        await self.astop()\n\n    @abstractmethod\n    def start(self) -&gt; None: ...\n\n    @abstractmethod\n    def stop(self) -&gt; None: ...\n\n    @abstractmethod\n    async def astart(self) -&gt; None: ...\n\n    @abstractmethod\n    async def astop(self) -&gt; None: ...\n\n    @abstractmethod\n    def get_page_source(self, url: str) -&gt; str:\n        \"\"\"Return page content from an URL\n\n        Args:\n            url: URL\n\n        Returns:\n            page content (html, json, whatever)\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def aget_page_source(self, url: str) -&gt; str:\n        \"\"\"Return page content from an URL asynchronously\n\n        Args:\n            url: URL\n\n        Returns:\n            page content (html, json, whatever)\n        \"\"\"\n        ...\n</code></pre>"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter.__aenter__","title":"<code>__aenter__()</code>  <code>async</code>","text":"<p>Async context manager enter method.</p> <p>Returns:</p> Type Description <code>PageSourceGetter</code> <p>Self instance for method chaining</p> Source code in <code>extract_emails/browsers/page_source_getter.py</code> <pre><code>async def __aenter__(self) -&gt; PageSourceGetter:\n    \"\"\"Async context manager enter method.\n\n    Returns:\n        Self instance for method chaining\n    \"\"\"\n    await self.astart()\n    return self\n</code></pre>"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter.__aexit__","title":"<code>__aexit__(exc_type, exc_val, exc_tb)</code>  <code>async</code>","text":"<p>Async context manager exit method.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <p>Exception type</p> required <code>exc_val</code> <p>Exception value</p> required <code>exc_tb</code> <p>Exception traceback</p> required Source code in <code>extract_emails/browsers/page_source_getter.py</code> <pre><code>async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Async context manager exit method.\n\n    Args:\n        exc_type: Exception type\n        exc_val: Exception value\n        exc_tb: Exception traceback\n    \"\"\"\n    await self.astop()\n</code></pre>"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager enter method.</p> <p>Returns:</p> Type Description <code>PageSourceGetter</code> <p>Self instance for method chaining</p> Source code in <code>extract_emails/browsers/page_source_getter.py</code> <pre><code>def __enter__(self) -&gt; PageSourceGetter:\n    \"\"\"Context manager enter method.\n\n    Returns:\n        Self instance for method chaining\n    \"\"\"\n    self.start()\n    return self\n</code></pre>"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit method.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <p>Exception type</p> required <code>exc_val</code> <p>Exception value</p> required <code>exc_tb</code> <p>Exception traceback</p> required Source code in <code>extract_emails/browsers/page_source_getter.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n    \"\"\"Context manager exit method.\n\n    Args:\n        exc_type: Exception type\n        exc_val: Exception value\n        exc_tb: Exception traceback\n    \"\"\"\n    self.stop()\n</code></pre>"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter.aget_page_source","title":"<code>aget_page_source(url)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return page content from an URL asynchronously</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL</p> required <p>Returns:</p> Type Description <code>str</code> <p>page content (html, json, whatever)</p> Source code in <code>extract_emails/browsers/page_source_getter.py</code> <pre><code>@abstractmethod\nasync def aget_page_source(self, url: str) -&gt; str:\n    \"\"\"Return page content from an URL asynchronously\n\n    Args:\n        url: URL\n\n    Returns:\n        page content (html, json, whatever)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter.get_page_source","title":"<code>get_page_source(url)</code>  <code>abstractmethod</code>","text":"<p>Return page content from an URL</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL</p> required <p>Returns:</p> Type Description <code>str</code> <p>page content (html, json, whatever)</p> Source code in <code>extract_emails/browsers/page_source_getter.py</code> <pre><code>@abstractmethod\ndef get_page_source(self, url: str) -&gt; str:\n    \"\"\"Return page content from an URL\n\n    Args:\n        url: URL\n\n    Returns:\n        page content (html, json, whatever)\n    \"\"\"\n    ...\n</code></pre>"},{"location":"code/data_extractors/","title":"Data Extractors","text":""},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor","title":"<code>DataExtractor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all data extractors</p> Source code in <code>extract_emails/data_extractors/data_extractor.py</code> <pre><code>class DataExtractor(ABC):\n    \"\"\"Base class for all data extractors\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Name of the data extractor, e.g. email, linkedin\"\"\"\n\n    @abstractmethod\n    def get_data(self, page_source: str) -&gt; set[str]:\n        \"\"\"Extract needed data from a string\n\n        Args:\n            page_source: webpage content\n\n        Returns:\n            Set of data, e.g. {'email@email.com', 'email2@email.com'}\n        \"\"\"\n</code></pre>"},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Name of the data extractor, e.g. email, linkedin</p>"},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor.get_data","title":"<code>get_data(page_source)</code>  <code>abstractmethod</code>","text":"<p>Extract needed data from a string</p> <p>Parameters:</p> Name Type Description Default <code>page_source</code> <code>str</code> <p>webpage content</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of data, e.g. {'email@email.com', 'email2@email.com'}</p> Source code in <code>extract_emails/data_extractors/data_extractor.py</code> <pre><code>@abstractmethod\ndef get_data(self, page_source: str) -&gt; set[str]:\n    \"\"\"Extract needed data from a string\n\n    Args:\n        page_source: webpage content\n\n    Returns:\n        Set of data, e.g. {'email@email.com', 'email2@email.com'}\n    \"\"\"\n</code></pre>"},{"location":"code/data_extractors/#extract_emails.data_extractors.email_extractor.EmailExtractor","title":"<code>EmailExtractor</code>","text":"<p>               Bases: <code>DataExtractor</code></p> Source code in <code>extract_emails/data_extractors/email_extractor.py</code> <pre><code>class EmailExtractor(DataExtractor):\n    def __init__(self):\n        self.regexp = re.compile(\n            r\"\"\"(?:[a-z0-9!#$%&amp;'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&amp;'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n        )\n\n    @property\n    def name(self) -&gt; str:\n        return \"email\"\n\n    def get_data(self, page_source: str) -&gt; set[str]:\n        \"\"\"Extract emails from a string\n\n        Args:\n            page_source: webpage content\n\n        Returns:\n            Set of emails, e.g. {'email@email.com', 'email2@email.com'}\n        \"\"\"\n        raw_emails = [i for i in self.regexp.findall(page_source)]\n        return email_filter(raw_emails)\n</code></pre>"},{"location":"code/data_extractors/#extract_emails.data_extractors.email_extractor.EmailExtractor.get_data","title":"<code>get_data(page_source)</code>","text":"<p>Extract emails from a string</p> <p>Parameters:</p> Name Type Description Default <code>page_source</code> <code>str</code> <p>webpage content</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of emails, e.g. {'email@email.com', 'email2@email.com'}</p> Source code in <code>extract_emails/data_extractors/email_extractor.py</code> <pre><code>def get_data(self, page_source: str) -&gt; set[str]:\n    \"\"\"Extract emails from a string\n\n    Args:\n        page_source: webpage content\n\n    Returns:\n        Set of emails, e.g. {'email@email.com', 'email2@email.com'}\n    \"\"\"\n    raw_emails = [i for i in self.regexp.findall(page_source)]\n    return email_filter(raw_emails)\n</code></pre>"},{"location":"code/data_extractors/#extract_emails.data_extractors.linkedin_extractor.LinkedinExtractor","title":"<code>LinkedinExtractor</code>","text":"<p>               Bases: <code>DataExtractor</code></p> Source code in <code>extract_emails/data_extractors/linkedin_extractor.py</code> <pre><code>class LinkedinExtractor(DataExtractor):\n    def __init__(self):\n        self.regexp = re.compile(\n            r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()&lt;&gt;]+|\\(([^\\s()&lt;&gt;]+|(\\([^\\s()&lt;&gt;]+\\)))*\\))+(?:\\(([^\\s()&lt;&gt;]+|(\\([^\\s()&lt;&gt;]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,&lt;&gt;?\u00ab\u00bb\u201c\u201d\u2018\u2019]))\"\n        )\n\n    @property\n    def name(self) -&gt; str:\n        return \"linkedin\"\n\n    def get_data(self, page_source: str) -&gt; set[str]:\n        \"\"\"Extract links to Linkedin profiles\n\n        Args:\n            page_source: webpage content\n\n        Returns:\n            Set of urls, e.g. {'https://www.linkedin.com/in/venjamin-brant-73381ujy3u'}\n        \"\"\"\n        all_urls = self.regexp.findall(page_source)\n        url_filter = \"linkedin.com/in/\"\n        linkedin_urls = set([i[0] for i in all_urls if url_filter in i[0]])\n        return linkedin_urls\n</code></pre>"},{"location":"code/data_extractors/#extract_emails.data_extractors.linkedin_extractor.LinkedinExtractor.get_data","title":"<code>get_data(page_source)</code>","text":"<p>Extract links to Linkedin profiles</p> <p>Parameters:</p> Name Type Description Default <code>page_source</code> <code>str</code> <p>webpage content</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of urls, e.g. {'https://www.linkedin.com/in/venjamin-brant-73381ujy3u'}</p> Source code in <code>extract_emails/data_extractors/linkedin_extractor.py</code> <pre><code>def get_data(self, page_source: str) -&gt; set[str]:\n    \"\"\"Extract links to Linkedin profiles\n\n    Args:\n        page_source: webpage content\n\n    Returns:\n        Set of urls, e.g. {'https://www.linkedin.com/in/venjamin-brant-73381ujy3u'}\n    \"\"\"\n    all_urls = self.regexp.findall(page_source)\n    url_filter = \"linkedin.com/in/\"\n    linkedin_urls = set([i[0] for i in all_urls if url_filter in i[0]])\n    return linkedin_urls\n</code></pre>"},{"location":"code/errors/","title":"Errors","text":""},{"location":"code/errors/#extract_emails.errors.errors.BrowserImportError","title":"<code>BrowserImportError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Error for cases when required libraries for browsers were not installed</p> Source code in <code>extract_emails/errors/errors.py</code> <pre><code>class BrowserImportError(Exception):\n    \"\"\"Error for cases when required libraries for browsers were not installed\"\"\"\n\n    pass\n</code></pre>"},{"location":"code/link_filters/","title":"Link Filters","text":""},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase","title":"<code>LinkFilterBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for link filters</p> Source code in <code>extract_emails/link_filters/link_filter_base.py</code> <pre><code>class LinkFilterBase(ABC):\n    \"\"\"Base class for link filters\"\"\"\n\n    def __init__(self, website: str):\n        \"\"\"\n\n        Args:\n            website: website address (scheme and domain), e.g. https://example.com\n        \"\"\"\n        self.website = website\n\n    @staticmethod\n    def get_website_address(url: str) -&gt; str:\n        \"\"\"Extract scheme and domain name from an URL\n\n        Examples:\n            &gt;&gt;&gt; from extract_emails.link_filters import LinkFilterBase\n            &gt;&gt;&gt; website = LinkFilterBase.get_website_address('https://example.com/list?page=134')\n            &gt;&gt;&gt; website\n            'https://example.com/'\n\n        Args:\n            url: URL for parsing\n\n        Returns:\n            scheme and domain name from URL, e.g. https://example.com\n\n        \"\"\"\n        parsed_url = urlparse(url)\n        return f\"{parsed_url.scheme}://{parsed_url.netloc}/\"\n\n    @staticmethod\n    def get_links(page_source: str) -&gt; list[str]:\n        \"\"\"Extract all URLs corresponding to current website\n\n        Examples:\n            &gt;&gt;&gt; from extract_emails.link_filters import LinkFilterBase\n            &gt;&gt;&gt; links = LinkFilterBase.get_links(page_source)\n            &gt;&gt;&gt; links\n            [\"example.com\", \"/example.com\", \"https://example2.com\"]\n\n        Args:\n            page_source: HTML page source\n\n        Returns:\n            List of URLs\n\n        :param str page_source: HTML page source\n        :return: List of URLs\n        \"\"\"\n        links = RE_LINKS.findall(page_source)\n        links = [x[1] for x in links]\n        return links\n\n    @abstractmethod\n    def filter(self, urls: Iterable[str]) -&gt; list[str]:\n        \"\"\"Filter links by some parameters\n\n        Args:\n            urls: List of URLs for filtering\n\n        Returns:\n            List of filtered URLs\n        \"\"\"\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.__init__","title":"<code>__init__(website)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>website</code> <code>str</code> <p>website address (scheme and domain), e.g. https://example.com</p> required Source code in <code>extract_emails/link_filters/link_filter_base.py</code> <pre><code>def __init__(self, website: str):\n    \"\"\"\n\n    Args:\n        website: website address (scheme and domain), e.g. https://example.com\n    \"\"\"\n    self.website = website\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.filter","title":"<code>filter(urls)</code>  <code>abstractmethod</code>","text":"<p>Filter links by some parameters</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>Iterable[str]</code> <p>List of URLs for filtering</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of filtered URLs</p> Source code in <code>extract_emails/link_filters/link_filter_base.py</code> <pre><code>@abstractmethod\ndef filter(self, urls: Iterable[str]) -&gt; list[str]:\n    \"\"\"Filter links by some parameters\n\n    Args:\n        urls: List of URLs for filtering\n\n    Returns:\n        List of filtered URLs\n    \"\"\"\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.get_links","title":"<code>get_links(page_source)</code>  <code>staticmethod</code>","text":"<p>Extract all URLs corresponding to current website</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from extract_emails.link_filters import LinkFilterBase\n&gt;&gt;&gt; links = LinkFilterBase.get_links(page_source)\n&gt;&gt;&gt; links\n[\"example.com\", \"/example.com\", \"https://example2.com\"]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>page_source</code> <code>str</code> <p>HTML page source</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of URLs</p> <p>:param str page_source: HTML page source :return: List of URLs</p> Source code in <code>extract_emails/link_filters/link_filter_base.py</code> <pre><code>@staticmethod\ndef get_links(page_source: str) -&gt; list[str]:\n    \"\"\"Extract all URLs corresponding to current website\n\n    Examples:\n        &gt;&gt;&gt; from extract_emails.link_filters import LinkFilterBase\n        &gt;&gt;&gt; links = LinkFilterBase.get_links(page_source)\n        &gt;&gt;&gt; links\n        [\"example.com\", \"/example.com\", \"https://example2.com\"]\n\n    Args:\n        page_source: HTML page source\n\n    Returns:\n        List of URLs\n\n    :param str page_source: HTML page source\n    :return: List of URLs\n    \"\"\"\n    links = RE_LINKS.findall(page_source)\n    links = [x[1] for x in links]\n    return links\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.get_website_address","title":"<code>get_website_address(url)</code>  <code>staticmethod</code>","text":"<p>Extract scheme and domain name from an URL</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from extract_emails.link_filters import LinkFilterBase\n&gt;&gt;&gt; website = LinkFilterBase.get_website_address('https://example.com/list?page=134')\n&gt;&gt;&gt; website\n'https://example.com/'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL for parsing</p> required <p>Returns:</p> Type Description <code>str</code> <p>scheme and domain name from URL, e.g. https://example.com</p> Source code in <code>extract_emails/link_filters/link_filter_base.py</code> <pre><code>@staticmethod\ndef get_website_address(url: str) -&gt; str:\n    \"\"\"Extract scheme and domain name from an URL\n\n    Examples:\n        &gt;&gt;&gt; from extract_emails.link_filters import LinkFilterBase\n        &gt;&gt;&gt; website = LinkFilterBase.get_website_address('https://example.com/list?page=134')\n        &gt;&gt;&gt; website\n        'https://example.com/'\n\n    Args:\n        url: URL for parsing\n\n    Returns:\n        scheme and domain name from URL, e.g. https://example.com\n\n    \"\"\"\n    parsed_url = urlparse(url)\n    return f\"{parsed_url.scheme}://{parsed_url.netloc}/\"\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.default_link_filter.DefaultLinkFilter","title":"<code>DefaultLinkFilter</code>","text":"<p>               Bases: <code>LinkFilterBase</code></p> <p>Default filter for links</p> Source code in <code>extract_emails/link_filters/default_link_filter.py</code> <pre><code>class DefaultLinkFilter(LinkFilterBase):\n    \"\"\"Default filter for links\"\"\"\n\n    def __init__(self, website: str):\n        super().__init__(website)\n        self.checked_links: set[str] = set()\n\n    def filter(self, links: Iterable[str]) -&gt; list[str]:\n        \"\"\"Will exclude from a list URLs, which not starts with `self.website` and not starts with '/'\n\n        Examples:\n            &gt;&gt;&gt; from extract_emails.link_filters import DefaultLinkFilter\n            &gt;&gt;&gt; test_urls = [\"https://example.com/page1.html\",\"/page.html\",\"/page.html\", \"https://google.com\"]\n            &gt;&gt;&gt; link_filter = DefaultLinkFilter(\"https://example.com/\")\n            &gt;&gt;&gt; filtered_urls = link_filter.filter(test_urls)\n            &gt;&gt;&gt; filtered_urls\n            [\"https://example.com/page1.html\", \"https://example.com/page.html\"]\n\n        Args:\n            links: List of links for filtering\n\n        Returns:\n            Set of filtered URLs\n        \"\"\"\n        filtered_urls = []\n        for link in links:\n            url = urljoin(self.website, link)\n            if not url.startswith(self.website):\n                continue\n            if url in self.checked_links:\n                continue\n            filtered_urls.append(url)\n            self.checked_links.add(url)\n\n        return filtered_urls\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.default_link_filter.DefaultLinkFilter.filter","title":"<code>filter(links)</code>","text":"<p>Will exclude from a list URLs, which not starts with <code>self.website</code> and not starts with '/'</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from extract_emails.link_filters import DefaultLinkFilter\n&gt;&gt;&gt; test_urls = [\"https://example.com/page1.html\",\"/page.html\",\"/page.html\", \"https://google.com\"]\n&gt;&gt;&gt; link_filter = DefaultLinkFilter(\"https://example.com/\")\n&gt;&gt;&gt; filtered_urls = link_filter.filter(test_urls)\n&gt;&gt;&gt; filtered_urls\n[\"https://example.com/page1.html\", \"https://example.com/page.html\"]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>links</code> <code>Iterable[str]</code> <p>List of links for filtering</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>Set of filtered URLs</p> Source code in <code>extract_emails/link_filters/default_link_filter.py</code> <pre><code>def filter(self, links: Iterable[str]) -&gt; list[str]:\n    \"\"\"Will exclude from a list URLs, which not starts with `self.website` and not starts with '/'\n\n    Examples:\n        &gt;&gt;&gt; from extract_emails.link_filters import DefaultLinkFilter\n        &gt;&gt;&gt; test_urls = [\"https://example.com/page1.html\",\"/page.html\",\"/page.html\", \"https://google.com\"]\n        &gt;&gt;&gt; link_filter = DefaultLinkFilter(\"https://example.com/\")\n        &gt;&gt;&gt; filtered_urls = link_filter.filter(test_urls)\n        &gt;&gt;&gt; filtered_urls\n        [\"https://example.com/page1.html\", \"https://example.com/page.html\"]\n\n    Args:\n        links: List of links for filtering\n\n    Returns:\n        Set of filtered URLs\n    \"\"\"\n    filtered_urls = []\n    for link in links:\n        url = urljoin(self.website, link)\n        if not url.startswith(self.website):\n            continue\n        if url in self.checked_links:\n            continue\n        filtered_urls.append(url)\n        self.checked_links.add(url)\n\n    return filtered_urls\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter","title":"<code>ContactInfoLinkFilter</code>","text":"<p>               Bases: <code>LinkFilterBase</code></p> <p>Contact information filter for links.</p> <p>Only keep the links might contain the contact information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from extract_emails.link_filters import ContactInfoLinkFilter\n&gt;&gt;&gt; link_filter = ContactInfoLinkFilter(\"https://example.com\")\n&gt;&gt;&gt; filtered_links = link_filter.filter(['/about-us', '/search'])\n&gt;&gt;&gt; filtered_links\n['https://example.com/about-us']\n</code></pre> <pre><code>&gt;&gt;&gt; from extract_emails.link_filters import ContactInfoLinkFilter\n&gt;&gt;&gt; link_filter = ContactInfoLinkFilter(\"https://example.com\", use_default=True)\n&gt;&gt;&gt; filtered_links = link_filter.filter(['/blog', '/search'])\n&gt;&gt;&gt; filtered_links\n['https://example.com/blog', 'https://example.com/search']\n</code></pre> <pre><code>&gt;&gt;&gt; from extract_emails.link_filters import ContactInfoLinkFilter\n&gt;&gt;&gt; link_filter = ContactInfoLinkFilter(\"https://example.com\", use_default=False)\n&gt;&gt;&gt; filtered_links = link_filter.filter(['/blog', '/search'])\n&gt;&gt;&gt; filtered_links\n[]\n</code></pre> <pre><code>&gt;&gt;&gt; from extract_emails.link_filters import ContactInfoLinkFilter\n&gt;&gt;&gt; link_filter = ContactInfoLinkFilter(\"https://example.com\", contruct_candidates=['search'])\n&gt;&gt;&gt; filtered_links = link_filter.filter(['/blog', '/search'])\n&gt;&gt;&gt; filtered_links\n['https://example.com/search']\n</code></pre> Source code in <code>extract_emails/link_filters/contact_link_filter.py</code> <pre><code>class ContactInfoLinkFilter(LinkFilterBase):\n    \"\"\"Contact information filter for links.\n\n    Only keep the links might contain the contact information.\n\n    Examples:\n        &gt;&gt;&gt; from extract_emails.link_filters import ContactInfoLinkFilter\n        &gt;&gt;&gt; link_filter = ContactInfoLinkFilter(\"https://example.com\")\n        &gt;&gt;&gt; filtered_links = link_filter.filter(['/about-us', '/search'])\n        &gt;&gt;&gt; filtered_links\n        ['https://example.com/about-us']\n\n\n        &gt;&gt;&gt; from extract_emails.link_filters import ContactInfoLinkFilter\n        &gt;&gt;&gt; link_filter = ContactInfoLinkFilter(\"https://example.com\", use_default=True)\n        &gt;&gt;&gt; filtered_links = link_filter.filter(['/blog', '/search'])\n        &gt;&gt;&gt; filtered_links\n        ['https://example.com/blog', 'https://example.com/search']\n\n        &gt;&gt;&gt; from extract_emails.link_filters import ContactInfoLinkFilter\n        &gt;&gt;&gt; link_filter = ContactInfoLinkFilter(\"https://example.com\", use_default=False)\n        &gt;&gt;&gt; filtered_links = link_filter.filter(['/blog', '/search'])\n        &gt;&gt;&gt; filtered_links\n        []\n\n        &gt;&gt;&gt; from extract_emails.link_filters import ContactInfoLinkFilter\n        &gt;&gt;&gt; link_filter = ContactInfoLinkFilter(\"https://example.com\", contruct_candidates=['search'])\n        &gt;&gt;&gt; filtered_links = link_filter.filter(['/blog', '/search'])\n        &gt;&gt;&gt; filtered_links\n        ['https://example.com/search']\n    \"\"\"\n\n    default_contruct_candidates = [\n        \"about\",\n        \"about-us\",\n        \"aboutus\",\n        \"contact\",\n        \"contact-us\",\n        \"contactus\",\n    ]\n\n    checked_links: set[str] = set()\n\n    def __init__(\n        self,\n        website: str,\n        contruct_candidates: list[str] | None = None,\n        use_default: bool = False,\n    ):\n        \"\"\"\n\n        Args:\n            website: website address (scheme and domain), e.g. https://example.com\n            contruct_candidates: keywords for filtering the list of URLs,\n                default: see `self.default_contruct_candidates`\n            use_default:  if no contactinfo urls found and return filtered_urls,\n                default: True\n        \"\"\"\n        super().__init__(website)\n        self.checked_links = set()\n        self.candidates = (\n            contruct_candidates\n            if contruct_candidates is not None\n            else self.default_contruct_candidates\n        )\n        self.use_default = use_default\n\n    def filter(self, urls: Iterable[str]) -&gt; list[str]:\n        \"\"\"Filter out the links without keywords\n\n        Args:\n            urls: List of URLs for filtering\n\n        Returns:\n            List of filtered URLs\n        \"\"\"\n        filtered_urls = []\n        contactinfo_urls = []\n\n        for url in urls:\n            url = urljoin(self.website, url)\n\n            if not url.startswith(self.website):\n                continue\n            if url in self.checked_links:\n                continue\n            filtered_urls.append(url)\n            self.checked_links.add(url)\n\n            for cand in self.candidates:\n                if cand in url.lower():\n                    contactinfo_urls.append(url)\n                    break\n\n        return (\n            filtered_urls\n            if len(contactinfo_urls) == 0 and self.use_default\n            else contactinfo_urls\n        )\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter.__init__","title":"<code>__init__(website, contruct_candidates=None, use_default=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>website</code> <code>str</code> <p>website address (scheme and domain), e.g. https://example.com</p> required <code>contruct_candidates</code> <code>list[str] | None</code> <p>keywords for filtering the list of URLs, default: see <code>self.default_contruct_candidates</code></p> <code>None</code> <code>use_default</code> <code>bool</code> <p>if no contactinfo urls found and return filtered_urls, default: True</p> <code>False</code> Source code in <code>extract_emails/link_filters/contact_link_filter.py</code> <pre><code>def __init__(\n    self,\n    website: str,\n    contruct_candidates: list[str] | None = None,\n    use_default: bool = False,\n):\n    \"\"\"\n\n    Args:\n        website: website address (scheme and domain), e.g. https://example.com\n        contruct_candidates: keywords for filtering the list of URLs,\n            default: see `self.default_contruct_candidates`\n        use_default:  if no contactinfo urls found and return filtered_urls,\n            default: True\n    \"\"\"\n    super().__init__(website)\n    self.checked_links = set()\n    self.candidates = (\n        contruct_candidates\n        if contruct_candidates is not None\n        else self.default_contruct_candidates\n    )\n    self.use_default = use_default\n</code></pre>"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter.filter","title":"<code>filter(urls)</code>","text":"<p>Filter out the links without keywords</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>Iterable[str]</code> <p>List of URLs for filtering</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of filtered URLs</p> Source code in <code>extract_emails/link_filters/contact_link_filter.py</code> <pre><code>def filter(self, urls: Iterable[str]) -&gt; list[str]:\n    \"\"\"Filter out the links without keywords\n\n    Args:\n        urls: List of URLs for filtering\n\n    Returns:\n        List of filtered URLs\n    \"\"\"\n    filtered_urls = []\n    contactinfo_urls = []\n\n    for url in urls:\n        url = urljoin(self.website, url)\n\n        if not url.startswith(self.website):\n            continue\n        if url in self.checked_links:\n            continue\n        filtered_urls.append(url)\n        self.checked_links.add(url)\n\n        for cand in self.candidates:\n            if cand in url.lower():\n                contactinfo_urls.append(url)\n                break\n\n    return (\n        filtered_urls\n        if len(contactinfo_urls) == 0 and self.use_default\n        else contactinfo_urls\n    )\n</code></pre>"},{"location":"code/models/","title":"Models","text":""},{"location":"code/models/#extract_emails.models.page_data.PageData","title":"<code>PageData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Representation for data from a webpage</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from extract_emails.models import PageData\n&gt;&gt;&gt; page_data = PageData(website='https://example.com', page_url='https://example.com/page123')\n</code></pre> <p>Attributes:</p> Name Type Description <code>website</code> <code>str</code> <p>website address from where data</p> <code>page_url</code> <code>str</code> <p>Page URL from where data</p> <code>data</code> <code>Optional[Dict[str, List[str]]]</code> <p>Data from the page in format: { 'label': [data, data] }, default: {}</p> Source code in <code>extract_emails/models/page_data.py</code> <pre><code>class PageData(BaseModel):\n    \"\"\"Representation for data from a webpage\n\n    Examples:\n        &gt;&gt;&gt; from extract_emails.models import PageData\n        &gt;&gt;&gt; page_data = PageData(website='https://example.com', page_url='https://example.com/page123')\n\n    Attributes:\n        website (str): website address from where data\n        page_url (str): Page URL from where data\n        data (Optional[Dict[str, List[str]]]): Data from the page in format: { 'label': [data, data] }, default: {}\n    \"\"\"\n\n    website: str\n    page_url: str\n    data: dict[str, list[str]] = Field(default_factory=dict)\n\n    def __len__(self) -&gt; int:\n        if len(self.data) == 0:\n            return 0\n        return sum(len(i) for i in self.data.values())\n\n    def append(self, label: str, vals: list[str]) -&gt; None:\n        \"\"\"Append data from a page to the self.data collection\n\n        Examples:\n            &gt;&gt;&gt; from extract_emails.models import PageData\n            &gt;&gt;&gt; page_data = PageData(website='https://example.com', page_url='https://example.com/page123')\n            &gt;&gt;&gt; page_data.append('email', ['email1@email.com', 'email2@email.com'])\n            &gt;&gt;&gt; page_data.page\n            &gt;&gt;&gt; {'email': ['email@email.com', 'email2@email.com']}\n\n        Args:\n            label: name of collection, e.g. email, linkedin\n            vals: data from a page, e.g. emails, specific URLs etc.\n        \"\"\"\n        try:\n            self.data[label].extend(vals)\n        except KeyError:\n            self.data[label] = vals\n\n    @classmethod\n    def to_csv(cls, data: list[\"PageData\"], filepath: Path) -&gt; None:\n        \"\"\"Save list of `PageData` to CSV file\n\n        Args:\n            data: list of `PageData`\n            filepath: path to a CSV file\n        \"\"\"\n        base_headers: list[str] = list(cls.model_json_schema()[\"properties\"].keys())\n        base_headers.remove(\"data\")\n        data_headers = [i for i in data[0].data.keys()]\n        headers = base_headers + data_headers\n        is_file_exists = filepath.exists()\n\n        with open(filepath, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n            writer = csv.DictWriter(f, fieldnames=headers)\n            if not is_file_exists:\n                writer.writeheader()\n            for page in data:\n                for data_in_row in zip_longest(*page.data.values()):\n                    new_row = {\"website\": page.website, \"page_url\": page.page_url}\n                    for counter, column in enumerate(data_headers):\n                        new_row[column] = data_in_row[counter]\n\n                    writer.writerow(new_row)\n\n    @classmethod\n    async def ato_csv(cls, data: list[\"PageData\"], filepath: Path) -&gt; None:\n        \"\"\"Async save list of `PageData` to CSV file\n\n        Args:\n            data: list of `PageData`\n            filepath: path to a CSV file\n        \"\"\"\n        base_headers: list[str] = list(cls.model_json_schema()[\"properties\"].keys())\n        base_headers.remove(\"data\")\n        data_headers = [i for i in data[0].data.keys()]\n        headers = base_headers + data_headers\n        is_file_exists = filepath.exists()\n\n        async with aiofiles.open(filepath, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n            writer = AsyncDictWriter(f, fieldnames=headers)\n            if not is_file_exists:\n                await writer.writeheader()\n            for page in data:\n                for data_in_row in zip_longest(*page.data.values()):\n                    new_row = {\"website\": page.website, \"page_url\": page.page_url}\n                    for counter, column in enumerate(data_headers):\n                        new_row[column] = data_in_row[counter]\n\n                    await writer.writerow(new_row)\n</code></pre>"},{"location":"code/models/#extract_emails.models.page_data.PageData.append","title":"<code>append(label, vals)</code>","text":"<p>Append data from a page to the self.data collection</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from extract_emails.models import PageData\n&gt;&gt;&gt; page_data = PageData(website='https://example.com', page_url='https://example.com/page123')\n&gt;&gt;&gt; page_data.append('email', ['email1@email.com', 'email2@email.com'])\n&gt;&gt;&gt; page_data.page\n&gt;&gt;&gt; {'email': ['email@email.com', 'email2@email.com']}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>name of collection, e.g. email, linkedin</p> required <code>vals</code> <code>list[str]</code> <p>data from a page, e.g. emails, specific URLs etc.</p> required Source code in <code>extract_emails/models/page_data.py</code> <pre><code>def append(self, label: str, vals: list[str]) -&gt; None:\n    \"\"\"Append data from a page to the self.data collection\n\n    Examples:\n        &gt;&gt;&gt; from extract_emails.models import PageData\n        &gt;&gt;&gt; page_data = PageData(website='https://example.com', page_url='https://example.com/page123')\n        &gt;&gt;&gt; page_data.append('email', ['email1@email.com', 'email2@email.com'])\n        &gt;&gt;&gt; page_data.page\n        &gt;&gt;&gt; {'email': ['email@email.com', 'email2@email.com']}\n\n    Args:\n        label: name of collection, e.g. email, linkedin\n        vals: data from a page, e.g. emails, specific URLs etc.\n    \"\"\"\n    try:\n        self.data[label].extend(vals)\n    except KeyError:\n        self.data[label] = vals\n</code></pre>"},{"location":"code/models/#extract_emails.models.page_data.PageData.ato_csv","title":"<code>ato_csv(data, filepath)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Async save list of <code>PageData</code> to CSV file</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[PageData]</code> <p>list of <code>PageData</code></p> required <code>filepath</code> <code>Path</code> <p>path to a CSV file</p> required Source code in <code>extract_emails/models/page_data.py</code> <pre><code>@classmethod\nasync def ato_csv(cls, data: list[\"PageData\"], filepath: Path) -&gt; None:\n    \"\"\"Async save list of `PageData` to CSV file\n\n    Args:\n        data: list of `PageData`\n        filepath: path to a CSV file\n    \"\"\"\n    base_headers: list[str] = list(cls.model_json_schema()[\"properties\"].keys())\n    base_headers.remove(\"data\")\n    data_headers = [i for i in data[0].data.keys()]\n    headers = base_headers + data_headers\n    is_file_exists = filepath.exists()\n\n    async with aiofiles.open(filepath, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n        writer = AsyncDictWriter(f, fieldnames=headers)\n        if not is_file_exists:\n            await writer.writeheader()\n        for page in data:\n            for data_in_row in zip_longest(*page.data.values()):\n                new_row = {\"website\": page.website, \"page_url\": page.page_url}\n                for counter, column in enumerate(data_headers):\n                    new_row[column] = data_in_row[counter]\n\n                await writer.writerow(new_row)\n</code></pre>"},{"location":"code/models/#extract_emails.models.page_data.PageData.to_csv","title":"<code>to_csv(data, filepath)</code>  <code>classmethod</code>","text":"<p>Save list of <code>PageData</code> to CSV file</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[PageData]</code> <p>list of <code>PageData</code></p> required <code>filepath</code> <code>Path</code> <p>path to a CSV file</p> required Source code in <code>extract_emails/models/page_data.py</code> <pre><code>@classmethod\ndef to_csv(cls, data: list[\"PageData\"], filepath: Path) -&gt; None:\n    \"\"\"Save list of `PageData` to CSV file\n\n    Args:\n        data: list of `PageData`\n        filepath: path to a CSV file\n    \"\"\"\n    base_headers: list[str] = list(cls.model_json_schema()[\"properties\"].keys())\n    base_headers.remove(\"data\")\n    data_headers = [i for i in data[0].data.keys()]\n    headers = base_headers + data_headers\n    is_file_exists = filepath.exists()\n\n    with open(filepath, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=headers)\n        if not is_file_exists:\n            writer.writeheader()\n        for page in data:\n            for data_in_row in zip_longest(*page.data.values()):\n                new_row = {\"website\": page.website, \"page_url\": page.page_url}\n                for counter, column in enumerate(data_headers):\n                    new_row[column] = data_in_row[counter]\n\n                writer.writerow(new_row)\n</code></pre>"},{"location":"code/utils/","title":"Utils","text":""},{"location":"code/utils/#extract_emails.utils.email_filter.email_filter","title":"<code>email_filter(emails)</code>","text":"<p>Remove duplicated emails and strings looks like emails (2@pic.png)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from extract_emails.utils import email_filter\n&gt;&gt;&gt; test_emails = [\"email@email.com\", \"email@email.com\", \"2@pic.png\"]\n&gt;&gt;&gt; filtered_emails = email_filter(test_emails)\n&gt;&gt;&gt; filtered_emails\n{\"email@email.com\"}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>emails</code> <code>Iterable[str]</code> <p>List of new emails</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>List of filtered emails</p> Source code in <code>extract_emails/utils/email_filter.py</code> <pre><code>def email_filter(emails: Iterable[str]) -&gt; set[str]:\n    \"\"\"Remove duplicated emails and strings looks like emails (2@pic.png)\n\n    Examples:\n        &gt;&gt;&gt; from extract_emails.utils import email_filter\n        &gt;&gt;&gt; test_emails = [\"email@email.com\", \"email@email.com\", \"2@pic.png\"]\n        &gt;&gt;&gt; filtered_emails = email_filter(test_emails)\n        &gt;&gt;&gt; filtered_emails\n        {\"email@email.com\"}\n\n    Args:\n        emails: List of new emails\n\n    Returns:\n        List of filtered emails\n    \"\"\"\n    return set(\n        email for email in emails if \".\" + email.split(\".\")[-1] in TOP_LEVEL_DOMAINS\n    )\n</code></pre>"},{"location":"code/workers/","title":"Workers","text":""},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker","title":"<code>DefaultWorker</code>","text":"<p>DefaultWorker is responsible for orchestrating the extraction of emails and LinkedIn URLs from a given website.</p> <p>This class utilizes both synchronous and asynchronous workers to perform the extraction process. It manages the configuration of the extraction process, including the website URL, browser, link filter, data extractors, depth, and maximum links to extract from a page.</p> <p>Attributes:</p> Name Type Description <code>website_url</code> <code>str</code> <p>The URL of the website to extract data from.</p> <code>browser</code> <code>PageSourceGetter</code> <p>The browser instance used to fetch page sources.</p> <code>link_filter</code> <code>LinkFilterBase</code> <p>The filter used to determine which links to follow.</p> <code>data_extractors</code> <code>list[DataExtractor]</code> <p>The list of data extractors to use for extracting information.</p> <code>depth</code> <code>int</code> <p>The maximum depth to traverse the website.</p> <code>max_links_from_page</code> <code>int</code> <p>The maximum number of links to extract from a single page.</p> <code>links</code> <code>list[list[str]]</code> <p>A list of lists containing URLs to be processed at each depth level.</p> <code>current_depth</code> <code>int</code> <p>The current depth level of the extraction process.</p> Source code in <code>extract_emails/workers/default_worker.py</code> <pre><code>class DefaultWorker:\n    \"\"\"DefaultWorker is responsible for orchestrating the extraction of emails and LinkedIn URLs from a given website.\n\n    This class utilizes both synchronous and asynchronous workers to perform the extraction process. It manages the\n    configuration of the extraction process, including the website URL, browser, link filter, data extractors, depth,\n    and maximum links to extract from a page.\n\n    Attributes:\n        website_url (str): The URL of the website to extract data from.\n        browser (PageSourceGetter): The browser instance used to fetch page sources.\n        link_filter (LinkFilterBase): The filter used to determine which links to follow.\n        data_extractors (list[DataExtractor]): The list of data extractors to use for extracting information.\n        depth (int): The maximum depth to traverse the website.\n        max_links_from_page (int): The maximum number of links to extract from a single page.\n        links (list[list[str]]): A list of lists containing URLs to be processed at each depth level.\n        current_depth (int): The current depth level of the extraction process.\n    \"\"\"\n\n    def __init__(\n        self,\n        website_url: str,\n        browser: PageSourceGetter,\n        *,\n        link_filter: LinkFilterBase | None = None,\n        data_extractors: list[DataExtractor] | None = None,\n        depth: int = 20,\n        max_links_from_page: int = 20,\n    ):\n        self.website_url = website_url.rstrip(\"/\")\n        self.browser = browser\n        self.link_filter = link_filter or ContactInfoLinkFilter(self.website_url)\n        self.data_extractors = data_extractors or [\n            EmailExtractor(),\n            LinkedinExtractor(),\n        ]\n        self.depth = depth\n        self.max_links_from_page = max_links_from_page\n\n        self.links = [[self.website_url]]\n        self.current_depth = 0\n\n        self._sync_worker = _SyncDefaultWorker(\n            self.website_url,\n            self.browser,\n            link_filter=self.link_filter,\n            data_extractors=self.data_extractors,\n            depth=self.depth,\n            max_links_from_page=self.max_links_from_page,\n        )\n        self._async_worker = _AsyncDefaultWorker(\n            self.website_url,\n            self.browser,\n            link_filter=self.link_filter,\n            data_extractors=self.data_extractors,\n            depth=self.depth,\n            max_links_from_page=self.max_links_from_page,\n        )\n\n    def get_data(self) -&gt; list[PageData]:\n        \"\"\"Retrieve extracted data synchronously.\n\n        Returns:\n            list[PageData]: A list of PageData objects containing the extracted information.\n        \"\"\"\n        return self._sync_worker.get_data()\n\n    async def aget_data(self) -&gt; list[PageData]:\n        \"\"\"Retrieve extracted data asynchronously.\n\n        Returns:\n            list[PageData]: A list of PageData objects containing the extracted information.\n        \"\"\"\n        return await self._async_worker.get_data()\n</code></pre>"},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker.aget_data","title":"<code>aget_data()</code>  <code>async</code>","text":"<p>Retrieve extracted data asynchronously.</p> <p>Returns:</p> Type Description <code>list[PageData]</code> <p>list[PageData]: A list of PageData objects containing the extracted information.</p> Source code in <code>extract_emails/workers/default_worker.py</code> <pre><code>async def aget_data(self) -&gt; list[PageData]:\n    \"\"\"Retrieve extracted data asynchronously.\n\n    Returns:\n        list[PageData]: A list of PageData objects containing the extracted information.\n    \"\"\"\n    return await self._async_worker.get_data()\n</code></pre>"},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker.get_data","title":"<code>get_data()</code>","text":"<p>Retrieve extracted data synchronously.</p> <p>Returns:</p> Type Description <code>list[PageData]</code> <p>list[PageData]: A list of PageData objects containing the extracted information.</p> Source code in <code>extract_emails/workers/default_worker.py</code> <pre><code>def get_data(self) -&gt; list[PageData]:\n    \"\"\"Retrieve extracted data synchronously.\n\n    Returns:\n        list[PageData]: A list of PageData objects containing the extracted information.\n    \"\"\"\n    return self._sync_worker.get_data()\n</code></pre>"},{"location":"quick_start/intro/","title":"Intro","text":""},{"location":"quick_start/intro/#installation","title":"Installation","text":"<pre><code>pip install extract_emails[all]\n# or\npip install extract_emails[httpx]\n# or\npip install extract_emails[playwright]\nplaywright install chromium --with-deps\n</code></pre>"},{"location":"quick_start/intro/#quick-usage","title":"Quick Usage","text":""},{"location":"quick_start/intro/#as-library","title":"As library","text":"<pre><code>from pathlib import Path\n\nfrom extract_emails import DefaultWorker\nfrom extract_emails.browsers import ChromiumBrowser, HttpxBrowser\nfrom extract_emails.models import PageData\n\ndef main():\n    with ChromiumBrowser() as browser:\n        worker = DefaultWorker(\"https://example.com, browser)\n        data = worker.get_data()\n        PageData.to_csv(data, Path(\"output.csv\"))\n\n    with HttpxBrowser() as browser:\n        worker = DefaultWorker(\"https://example.com, browser)\n        data = worker.get_data()\n        PageData.to_csv(data, Path(\"output.csv\"))\n\nasync def main():\n    async with ChromiumBrowser() as browser:\n        worker = DefaultWorker(\"https://example.com, browser)\n        data = await worker.aget_data()\n        await PageData.to_csv(data, Path(\"output.csv\"))\n\n    async with HttpxBrowser() as browser:\n        worker = DefaultWorker(\"https://example.com, browser)\n        data = await worker.aget_data()\n        await PageData.to_csv(data, Path(\"output.csv\"))\n</code></pre>"},{"location":"quick_start/intro/#as-cli-tool","title":"As CLI tool","text":"<p><pre><code>$ extract-emails --help\n\n$ extract-emails --url https://en.wikipedia.org/wiki/Email -of output.csv\n$ cat output.csv\nemail,page,website\nbob@b.org,https://en.wikipedia.org/wiki/Email,https://en.wikipedia.org/wiki/Email\n</code></pre> There are several main parts in the framework:</p> <ul> <li>browser - Class to navigate through specific website and extract data from the webpages (httpx, playwright etc.)</li> <li>link filter - Class to extract URLs from a page corresponding to the website. There are two link filters (<code>ContactInfoLinkFilter</code> by default):<ul> <li><code>DefaultLinkFilter</code> - Will extract all URLs corresponding to the website</li> <li><code>ContactInfoLinkFilter</code> - Will extract only contact URLs, e.g. /contact/, /about-us/ etc</li> </ul> </li> <li>data extractor - Class to extract data from a page. At the moment there are two data extractors (both by default):<ul> <li><code>EmailExtractor</code> - Will extract all emails from the page</li> <li><code>LinkedinExtractor</code> - Will extract all links to Linkedin profiles from the page</li> </ul> </li> <li><code>DefaultWorker</code> - All data extractions goes here</li> </ul>"},{"location":"quick_start/logs/","title":"Logs","text":"<p>There is loguru library under the hood.</p>"},{"location":"quick_start/logs/#settings","title":"Settings","text":"<pre><code>import sys\n\nfrom loguru import logger\n\nlogger.add(\n    sys.stderr,\n    format=\"{time} {level} {message}\",\n    filter=\"my_module\",\n    level=\"INFO\",\n)\n</code></pre>"},{"location":"quick_start/logs/#disableenable","title":"Disable/Enable","text":"<pre><code>from loguru import logger\n\nlogger.disable('extract_emails')\nlogger.enable('extract_emails')\n</code></pre>"},{"location":"quick_start/save_data/","title":"Save Data","text":"<p>Data store as pydantic models</p>"},{"location":"quick_start/save_data/#save-as-csv","title":"Save as CSV","text":"<pre><code>from extract_emails import DefaultFilterAndEmailFactory as Factory\nfrom extract_emails.browsers.requests_browser import RequestsBrowser as Browser\nfrom extract_emails.models import PageData\nfrom extract_emails.workers import DefaultWorker\n\n\nbrowser = Browser()\nextractor = DefaultWorker(\"https://example.com\", browser)\ndata = extractor.get_data()\n\nPageData.save_as_csv(data, \"output.csv\")\n# cat output.csv\nwebsite,page_url,email\nhttps://example.com,https://example.com/about-us,email@example.com\nhttps://example.com,https://example.com/about-us,email1@example.com\nhttps://example.com,https://example.com/about-us,email2@example.com\nhttps://example.com,https://example.com/about-us,email3@example.com\n</code></pre>"}]}