{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to extract-emails documentation \u00b6","title":"Welcome to extract-emails documentation"},{"location":"#welcome-to-extract-emails-documentation","text":"","title":"Welcome to extract-emails documentation"},{"location":"changelogs/v5/","text":"V5 \u00b6 5.0.2 \u00b6 Fix imports for factories and DefaultWorker","title":"Changelogs"},{"location":"changelogs/v5/#v5","text":"","title":"V5"},{"location":"changelogs/v5/#502","text":"Fix imports for factories and DefaultWorker","title":"5.0.2"},{"location":"code/browsers/","text":"Browsers \u00b6 PageSourceGetter \u00b6 All browsers must inherit from this class get_page_source ( self , url ) \u00b6 Return page content from an URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page content (html, json, whatever) Source code in extract_emails/browsers/page_source_getter.py @abstractmethod def get_page_source ( self , url : str ) -> str : \"\"\"Return page content from an URL Args: url: URL Returns: page content (html, json, whatever) \"\"\" ChromeBrowser \u00b6 Getting page sources with selenium and chromedriver Examples: >>> from extract_emails.browsers.chrome_browser import ChromeBrowser >>> browser = ChromeBrowser () >>> browser . open () >>> page_source = browser . get_page_source ( 'https://example.com' ) >>> browser . close () >>> from extract_emails.browsers.chrome_browser import ChromeBrowser >>> with ChromeBrowser () as browser : ... page_source = browser . get_page_source ( 'https://example.com' ) __init__ ( self , executable_path = '/usr/bin/chromedriver' , headless_mode = True , options = None ) special \u00b6 ChromeBrowser initialization Parameters: Name Type Description Default executable_path PathLike path to chromedriver, use which chromedriver to get the path. Default: /usr/bin/chromedriver '/usr/bin/chromedriver' headless_mode bool run browser with headless mode or not. Default: True True options Iterable[str] arguments for chrome.Options(). Default: set(\"--disable-gpu\", \"--disable-software-rasterizer\", \"--disable-dev-shm-usage\", \"--window-size=1920x1080\", \"--disable-setuid-sandbox\", \"--no-sandbox\", ) None Source code in extract_emails/browsers/chrome_browser.py def __init__ ( self , executable_path : PathLike = \"/usr/bin/chromedriver\" , headless_mode : bool = True , options : Iterable [ str ] = None , ) -> None : \"\"\"ChromeBrowser initialization Args: executable_path: path to chromedriver, use `which chromedriver` to get the path. Default: /usr/bin/chromedriver headless_mode: run browser with headless mode or not. Default: True options: arguments for chrome.Options(). Default: set(\"--disable-gpu\", \"--disable-software-rasterizer\", \"--disable-dev-shm-usage\", \"--window-size=1920x1080\", \"--disable-setuid-sandbox\", \"--no-sandbox\", ) \"\"\" self . executable_path = executable_path self . headless_mode = headless_mode self . options = options if options is not None else self . default_options self . driver : Optional [ webdriver . Chrome ] = None close ( self ) \u00b6 Close the browser Source code in extract_emails/browsers/chrome_browser.py def close ( self ): \"\"\"Close the browser\"\"\" self . driver . close () self . driver . quit () get_page_source ( self , url ) \u00b6 Get page source text from URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page source as text Source code in extract_emails/browsers/chrome_browser.py def get_page_source ( self , url : str ) -> str : \"\"\"Get page source text from URL Args: url: URL Returns: page source as text \"\"\" try : self . driver . get ( url ) time . sleep ( self . wait_seconds_after_get ) page_source = self . driver . page_source except Exception as e : logger . error ( f \"Could not get page source from { url } : { e } \" ) return \"\" if \"<html><head></head><body></body></html>\" == page_source : logger . error ( f \"Could not get page source from { url } : Unknown reason\" ) return page_source open ( self ) \u00b6 Add arguments to chrome.Options() and run the browser Source code in extract_emails/browsers/chrome_browser.py def open ( self ): \"\"\"Add arguments to chrome.Options() and run the browser\"\"\" options = Options () for option in self . options : options . add_argument ( option ) if self . headless_mode : options . add_argument ( \"--headless\" ) self . driver = webdriver . Chrome ( options = options , executable_path = self . executable_path ) RequestsBrowser \u00b6 Wrapper on requests library Examples: >>> from extract_emails.browsers.requests_browser import RequestsBrowser >>> browser = RequestsBrowser () >>> page_source = browser . get_page_source ( 'https://example.com' ) __init__ ( self , headers = None ) special \u00b6 Parameters: Name Type Description Default headers Dict[str, Any] headers for requests None Source code in extract_emails/browsers/requests_browser.py def __init__ ( self , headers : Dict [ str , Any ] = None ): \"\"\" Args: headers: headers for requests \"\"\" self . headers = headers self . session = requests . Session () get_page_source ( self , url ) \u00b6 Get page source text from URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page source as text Source code in extract_emails/browsers/requests_browser.py def get_page_source ( self , url : str ) -> str : \"\"\"Get page source text from URL Args: url: URL Returns: page source as text \"\"\" try : response = requests . get ( url , headers = self . headers ) except Exception as e : logger . error ( f \"Could not get page source from { url } : { e } \" ) return \"\" return response . text","title":"Browsers"},{"location":"code/browsers/#browsers","text":"","title":"Browsers"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter","text":"All browsers must inherit from this class","title":"PageSourceGetter"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter.get_page_source","text":"Return page content from an URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page content (html, json, whatever) Source code in extract_emails/browsers/page_source_getter.py @abstractmethod def get_page_source ( self , url : str ) -> str : \"\"\"Return page content from an URL Args: url: URL Returns: page content (html, json, whatever) \"\"\"","title":"get_page_source()"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser","text":"Getting page sources with selenium and chromedriver Examples: >>> from extract_emails.browsers.chrome_browser import ChromeBrowser >>> browser = ChromeBrowser () >>> browser . open () >>> page_source = browser . get_page_source ( 'https://example.com' ) >>> browser . close () >>> from extract_emails.browsers.chrome_browser import ChromeBrowser >>> with ChromeBrowser () as browser : ... page_source = browser . get_page_source ( 'https://example.com' )","title":"ChromeBrowser"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser.__init__","text":"ChromeBrowser initialization Parameters: Name Type Description Default executable_path PathLike path to chromedriver, use which chromedriver to get the path. Default: /usr/bin/chromedriver '/usr/bin/chromedriver' headless_mode bool run browser with headless mode or not. Default: True True options Iterable[str] arguments for chrome.Options(). Default: set(\"--disable-gpu\", \"--disable-software-rasterizer\", \"--disable-dev-shm-usage\", \"--window-size=1920x1080\", \"--disable-setuid-sandbox\", \"--no-sandbox\", ) None Source code in extract_emails/browsers/chrome_browser.py def __init__ ( self , executable_path : PathLike = \"/usr/bin/chromedriver\" , headless_mode : bool = True , options : Iterable [ str ] = None , ) -> None : \"\"\"ChromeBrowser initialization Args: executable_path: path to chromedriver, use `which chromedriver` to get the path. Default: /usr/bin/chromedriver headless_mode: run browser with headless mode or not. Default: True options: arguments for chrome.Options(). Default: set(\"--disable-gpu\", \"--disable-software-rasterizer\", \"--disable-dev-shm-usage\", \"--window-size=1920x1080\", \"--disable-setuid-sandbox\", \"--no-sandbox\", ) \"\"\" self . executable_path = executable_path self . headless_mode = headless_mode self . options = options if options is not None else self . default_options self . driver : Optional [ webdriver . Chrome ] = None","title":"__init__()"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser.close","text":"Close the browser Source code in extract_emails/browsers/chrome_browser.py def close ( self ): \"\"\"Close the browser\"\"\" self . driver . close () self . driver . quit ()","title":"close()"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser.get_page_source","text":"Get page source text from URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page source as text Source code in extract_emails/browsers/chrome_browser.py def get_page_source ( self , url : str ) -> str : \"\"\"Get page source text from URL Args: url: URL Returns: page source as text \"\"\" try : self . driver . get ( url ) time . sleep ( self . wait_seconds_after_get ) page_source = self . driver . page_source except Exception as e : logger . error ( f \"Could not get page source from { url } : { e } \" ) return \"\" if \"<html><head></head><body></body></html>\" == page_source : logger . error ( f \"Could not get page source from { url } : Unknown reason\" ) return page_source","title":"get_page_source()"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser.open","text":"Add arguments to chrome.Options() and run the browser Source code in extract_emails/browsers/chrome_browser.py def open ( self ): \"\"\"Add arguments to chrome.Options() and run the browser\"\"\" options = Options () for option in self . options : options . add_argument ( option ) if self . headless_mode : options . add_argument ( \"--headless\" ) self . driver = webdriver . Chrome ( options = options , executable_path = self . executable_path )","title":"open()"},{"location":"code/browsers/#extract_emails.browsers.requests_browser.RequestsBrowser","text":"Wrapper on requests library Examples: >>> from extract_emails.browsers.requests_browser import RequestsBrowser >>> browser = RequestsBrowser () >>> page_source = browser . get_page_source ( 'https://example.com' )","title":"RequestsBrowser"},{"location":"code/browsers/#extract_emails.browsers.requests_browser.RequestsBrowser.__init__","text":"Parameters: Name Type Description Default headers Dict[str, Any] headers for requests None Source code in extract_emails/browsers/requests_browser.py def __init__ ( self , headers : Dict [ str , Any ] = None ): \"\"\" Args: headers: headers for requests \"\"\" self . headers = headers self . session = requests . Session ()","title":"__init__()"},{"location":"code/browsers/#extract_emails.browsers.requests_browser.RequestsBrowser.get_page_source","text":"Get page source text from URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page source as text Source code in extract_emails/browsers/requests_browser.py def get_page_source ( self , url : str ) -> str : \"\"\"Get page source text from URL Args: url: URL Returns: page source as text \"\"\" try : response = requests . get ( url , headers = self . headers ) except Exception as e : logger . error ( f \"Could not get page source from { url } : { e } \" ) return \"\" return response . text","title":"get_page_source()"},{"location":"code/data_extractors/","text":"Data Extractors \u00b6 DataExtractor \u00b6 Base class for all data extractors name : str property readonly \u00b6 Name of the data extractor, e.g. email, linkedin get_data ( self , page_source ) \u00b6 Extract needed data from a string Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of data, e.g. {' email@email.com ', ' email2@email.com '} Source code in extract_emails/data_extractors/data_extractor.py @abstractmethod def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract needed data from a string Args: page_source: webpage content Returns: Set of data, e.g. {'email@email.com', 'email2@email.com'} \"\"\" EmailExtractor \u00b6 name : str property readonly \u00b6 Name of the data extractor, e.g. email, linkedin get_data ( self , page_source ) \u00b6 Extract emails from a string Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of emails, e.g. {' email@email.com ', ' email2@email.com '} Source code in extract_emails/data_extractors/email_extractor.py def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract emails from a string Args: page_source: webpage content Returns: Set of emails, e.g. {'email@email.com', 'email2@email.com'} \"\"\" raw_emails = [ i for i in self . regexp . findall ( page_source )] return email_filter ( raw_emails ) LinkedinExtractor \u00b6 name : str property readonly \u00b6 Name of the data extractor, e.g. email, linkedin get_data ( self , page_source ) \u00b6 Extract links to Linkedin profiles Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of urls, e.g. {' https://www.linkedin.com/in/venjamin-brant-73381ujy3u '} Source code in extract_emails/data_extractors/linkedin_extractor.py def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract links to Linkedin profiles Args: page_source: webpage content Returns: Set of urls, e.g. {'https://www.linkedin.com/in/venjamin-brant-73381ujy3u'} \"\"\" all_urls = self . regexp . findall ( page_source ) url_filter = \"linkedin.com/in/\" linkedin_urls = set ([ i [ 0 ] for i in all_urls if url_filter in i [ 0 ]]) return linkedin_urls","title":"Data Extractors"},{"location":"code/data_extractors/#data-extractors","text":"","title":"Data Extractors"},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor","text":"Base class for all data extractors","title":"DataExtractor"},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor.name","text":"Name of the data extractor, e.g. email, linkedin","title":"name"},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor.get_data","text":"Extract needed data from a string Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of data, e.g. {' email@email.com ', ' email2@email.com '} Source code in extract_emails/data_extractors/data_extractor.py @abstractmethod def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract needed data from a string Args: page_source: webpage content Returns: Set of data, e.g. {'email@email.com', 'email2@email.com'} \"\"\"","title":"get_data()"},{"location":"code/data_extractors/#extract_emails.data_extractors.email_extractor.EmailExtractor","text":"","title":"EmailExtractor"},{"location":"code/data_extractors/#extract_emails.data_extractors.email_extractor.EmailExtractor.name","text":"Name of the data extractor, e.g. email, linkedin","title":"name"},{"location":"code/data_extractors/#extract_emails.data_extractors.email_extractor.EmailExtractor.get_data","text":"Extract emails from a string Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of emails, e.g. {' email@email.com ', ' email2@email.com '} Source code in extract_emails/data_extractors/email_extractor.py def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract emails from a string Args: page_source: webpage content Returns: Set of emails, e.g. {'email@email.com', 'email2@email.com'} \"\"\" raw_emails = [ i for i in self . regexp . findall ( page_source )] return email_filter ( raw_emails )","title":"get_data()"},{"location":"code/data_extractors/#extract_emails.data_extractors.linkedin_extractor.LinkedinExtractor","text":"","title":"LinkedinExtractor"},{"location":"code/data_extractors/#extract_emails.data_extractors.linkedin_extractor.LinkedinExtractor.name","text":"Name of the data extractor, e.g. email, linkedin","title":"name"},{"location":"code/data_extractors/#extract_emails.data_extractors.linkedin_extractor.LinkedinExtractor.get_data","text":"Extract links to Linkedin profiles Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of urls, e.g. {' https://www.linkedin.com/in/venjamin-brant-73381ujy3u '} Source code in extract_emails/data_extractors/linkedin_extractor.py def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract links to Linkedin profiles Args: page_source: webpage content Returns: Set of urls, e.g. {'https://www.linkedin.com/in/venjamin-brant-73381ujy3u'} \"\"\" all_urls = self . regexp . findall ( page_source ) url_filter = \"linkedin.com/in/\" linkedin_urls = set ([ i [ 0 ] for i in all_urls if url_filter in i [ 0 ]]) return linkedin_urls","title":"get_data()"},{"location":"code/factories/","text":"Factories \u00b6 BaseFactory \u00b6 Base class for all factories data_extractors : List [ Type [ extract_emails . data_extractors . data_extractor . DataExtractor ]] property readonly \u00b6 Initialize data extractors link_filter : Type [ extract_emails . link_filters . link_filter_base . LinkFilterBase ] property readonly \u00b6 Initialize link filter __init__ ( self , * , website_url , browser , depth = None , max_links_from_page = None ) special \u00b6 Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10 None max_links_from_page Optional[int] how many links a script shall get from each page, default None (all) None Source code in extract_emails/factories/base_factory.py def __init__ ( self , * , website_url : str , browser : PageSourceGetter , depth : Optional [ int ] = None , max_links_from_page : Optional [ int ] = None , ): \"\"\" Args: website_url: website for scan, e.g. https://example.com browser: browser to get page source by URL depth: scan's depth, default 10 max_links_from_page: how many links a script shall get from each page, default None (all) \"\"\" self . _website_url = website_url self . _browser = browser self . _depth = depth self . _max_links_from_page = max_links_from_page DefaultFilterAndEmailFactory \u00b6 Will initialize DefaultLinkFilter and EmailExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndEmailFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe2.\"@example.com' , 'x2@example.com' ]} ), ] data_extractors : List [ extract_emails . data_extractors . email_extractor . EmailExtractor ] property readonly \u00b6 Initialize EmailExtractor link_filter : DefaultLinkFilter property readonly \u00b6 Initialize DefaultLinkFilter DefaultFilterAndLinkedinFactory \u00b6 Will initialize DefaultLinkFilter and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'linkedin' : [ 'linkeding profile url 1' , 'linkeding profile url 2' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'linkedin' : [ 'linkeding profile url 3' , 'linkeding profile url 4' ]} ), ] data_extractors : List [ extract_emails . data_extractors . linkedin_extractor . LinkedinExtractor ] property readonly \u00b6 Initialize LinkedinExtractor link_filter : DefaultLinkFilter property readonly \u00b6 Initialize DefaultLinkFilter DefaultFilterAndEmailAndLinkedinFactory \u00b6 Will initialize DefaultLinkFilter and EmailExtractor and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndEmailAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url1' , 'linkedin_url2' ], } ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url3' , 'linkedin_url4' ], } ), ] data_extractors : List [ Union [ extract_emails . data_extractors . email_extractor . EmailExtractor , extract_emails . data_extractors . linkedin_extractor . LinkedinExtractor ]] property readonly \u00b6 Initialize EmailExtractor and LinkedinExtractor link_filter : DefaultLinkFilter property readonly \u00b6 Initialize DefaultLinkFilter ContactFilterAndEmailFactory \u00b6 Will initialize ContactInfoLinkFilter and EmailExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndEmailFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe2.\"@example.com' , 'x2@example.com' ]} ), ] data_extractors : List [ extract_emails . data_extractors . email_extractor . EmailExtractor ] property readonly \u00b6 Initialize EmailExtractor link_filter : ContactInfoLinkFilter property readonly \u00b6 Initialize ContactInfoLinkFilter ContactFilterAndLinkedinFactory \u00b6 Will initialize ContactInfoLinkFilter and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'linkedin' : [ 'linkeding profile url 1' , 'linkeding profile url 2' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'linkedin' : [ 'linkeding profile url 3' , 'linkeding profile url 4' ]} ), ] data_extractors : List [ extract_emails . data_extractors . linkedin_extractor . LinkedinExtractor ] property readonly \u00b6 Initialize LinkedinExtractor link_filter : ContactInfoLinkFilter property readonly \u00b6 Initialize ContactInfoLinkFilter ContactFilterAndEmailAndLinkedinFactory \u00b6 Will initialize ContactInfoLinkFilter and EmailExtractor and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndEmailAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url1' , 'linkedin_url2' ], } ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url3' , 'linkedin_url4' ], } ), ] data_extractors : List [ Union [ extract_emails . data_extractors . email_extractor . EmailExtractor , extract_emails . data_extractors . linkedin_extractor . LinkedinExtractor ]] property readonly \u00b6 Initialize EmailExtractor and LinkedinExtractor link_filter : ContactInfoLinkFilter property readonly \u00b6 Initialize DefaultLinkFilter","title":"Factories"},{"location":"code/factories/#factories","text":"","title":"Factories"},{"location":"code/factories/#extract_emails.factories.base_factory.BaseFactory","text":"Base class for all factories","title":"BaseFactory"},{"location":"code/factories/#extract_emails.factories.base_factory.BaseFactory.data_extractors","text":"Initialize data extractors","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.base_factory.BaseFactory.link_filter","text":"Initialize link filter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.base_factory.BaseFactory.__init__","text":"Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10 None max_links_from_page Optional[int] how many links a script shall get from each page, default None (all) None Source code in extract_emails/factories/base_factory.py def __init__ ( self , * , website_url : str , browser : PageSourceGetter , depth : Optional [ int ] = None , max_links_from_page : Optional [ int ] = None , ): \"\"\" Args: website_url: website for scan, e.g. https://example.com browser: browser to get page source by URL depth: scan's depth, default 10 max_links_from_page: how many links a script shall get from each page, default None (all) \"\"\" self . _website_url = website_url self . _browser = browser self . _depth = depth self . _max_links_from_page = max_links_from_page","title":"__init__()"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email.DefaultFilterAndEmailFactory","text":"Will initialize DefaultLinkFilter and EmailExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndEmailFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe2.\"@example.com' , 'x2@example.com' ]} ), ]","title":"DefaultFilterAndEmailFactory"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email.DefaultFilterAndEmailFactory.data_extractors","text":"Initialize EmailExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email.DefaultFilterAndEmailFactory.link_filter","text":"Initialize DefaultLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.default_filter_and_linkedin.DefaultFilterAndLinkedinFactory","text":"Will initialize DefaultLinkFilter and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'linkedin' : [ 'linkeding profile url 1' , 'linkeding profile url 2' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'linkedin' : [ 'linkeding profile url 3' , 'linkeding profile url 4' ]} ), ]","title":"DefaultFilterAndLinkedinFactory"},{"location":"code/factories/#extract_emails.factories.default_filter_and_linkedin.DefaultFilterAndLinkedinFactory.data_extractors","text":"Initialize LinkedinExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.default_filter_and_linkedin.DefaultFilterAndLinkedinFactory.link_filter","text":"Initialize DefaultLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email_and_linkedin.DefaultFilterAndEmailAndLinkedinFactory","text":"Will initialize DefaultLinkFilter and EmailExtractor and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndEmailAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url1' , 'linkedin_url2' ], } ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url3' , 'linkedin_url4' ], } ), ]","title":"DefaultFilterAndEmailAndLinkedinFactory"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email_and_linkedin.DefaultFilterAndEmailAndLinkedinFactory.data_extractors","text":"Initialize EmailExtractor and LinkedinExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email_and_linkedin.DefaultFilterAndEmailAndLinkedinFactory.link_filter","text":"Initialize DefaultLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email.ContactFilterAndEmailFactory","text":"Will initialize ContactInfoLinkFilter and EmailExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndEmailFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe2.\"@example.com' , 'x2@example.com' ]} ), ]","title":"ContactFilterAndEmailFactory"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email.ContactFilterAndEmailFactory.data_extractors","text":"Initialize EmailExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email.ContactFilterAndEmailFactory.link_filter","text":"Initialize ContactInfoLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_linkedin.ContactFilterAndLinkedinFactory","text":"Will initialize ContactInfoLinkFilter and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'linkedin' : [ 'linkeding profile url 1' , 'linkeding profile url 2' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'linkedin' : [ 'linkeding profile url 3' , 'linkeding profile url 4' ]} ), ]","title":"ContactFilterAndLinkedinFactory"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_linkedin.ContactFilterAndLinkedinFactory.data_extractors","text":"Initialize LinkedinExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_linkedin.ContactFilterAndLinkedinFactory.link_filter","text":"Initialize ContactInfoLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email_and_linkedin.ContactFilterAndEmailAndLinkedinFactory","text":"Will initialize ContactInfoLinkFilter and EmailExtractor and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndEmailAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url1' , 'linkedin_url2' ], } ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url3' , 'linkedin_url4' ], } ), ]","title":"ContactFilterAndEmailAndLinkedinFactory"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email_and_linkedin.ContactFilterAndEmailAndLinkedinFactory.data_extractors","text":"Initialize EmailExtractor and LinkedinExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email_and_linkedin.ContactFilterAndEmailAndLinkedinFactory.link_filter","text":"Initialize DefaultLinkFilter","title":"link_filter"},{"location":"code/link_filters/","text":"Link Filters \u00b6 LinkFilterBase \u00b6 Base class for link filters __init__ ( self , website ) special \u00b6 Parameters: Name Type Description Default website str website address (scheme and domain), e.g. https://example.com required Source code in extract_emails/link_filters/link_filter_base.py def __init__ ( self , website : str ): \"\"\" Args: website: website address (scheme and domain), e.g. https://example.com \"\"\" self . website = website filter ( self , urls ) \u00b6 Filter links by some parameters Parameters: Name Type Description Default urls Iterable[str] List of URLs for filtering required Returns: Type Description List[str] List of filtered URLs Source code in extract_emails/link_filters/link_filter_base.py @abstractmethod def filter ( self , urls : Iterable [ str ]) -> List [ str ]: \"\"\"Filter links by some parameters Args: urls: List of URLs for filtering Returns: List of filtered URLs \"\"\" get_links ( page_source ) staticmethod \u00b6 Extract all URLs corresponding to current website Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> links = LinkFilterBase . get_links ( page_source ) >>> links [ \"example.com\" , \"/example.com\" , \"https://example2.com\" ] Parameters: Name Type Description Default page_source str HTML page source required Returns: Type Description List[str] List of URLs :param str page_source: HTML page source :return: List of URLs Source code in extract_emails/link_filters/link_filter_base.py @staticmethod def get_links ( page_source : str ) -> List [ str ]: \"\"\"Extract all URLs corresponding to current website Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> links = LinkFilterBase.get_links(page_source) >>> links [\"example.com\", \"/example.com\", \"https://example2.com\"] Args: page_source: HTML page source Returns: List of URLs :param str page_source: HTML page source :return: List of URLs \"\"\" links = RE_LINKS . findall ( page_source ) links = [ x [ 1 ] for x in links ] return links get_website_address ( url ) staticmethod \u00b6 Extract scheme and domain name from an URL Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> website = LinkFilterBase . get_website_address ( 'https://example.com/list?page=134' ) >>> website 'https://example.com/' Parameters: Name Type Description Default url str URL for parsing required Returns: Type Description str scheme and domain name from URL, e.g. https://example.com Source code in extract_emails/link_filters/link_filter_base.py @staticmethod def get_website_address ( url : str ) -> str : \"\"\"Extract scheme and domain name from an URL Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> website = LinkFilterBase.get_website_address('https://example.com/list?page=134') >>> website 'https://example.com/' Args: url: URL for parsing Returns: scheme and domain name from URL, e.g. https://example.com \"\"\" parsed_url = urlparse ( url ) return f \" { parsed_url . scheme } :// { parsed_url . netloc } /\" DefaultLinkFilter \u00b6 Default filter for links filter ( self , links ) \u00b6 Will exclude from a list URLs, which not starts with self.website and not starts with '/' Examples: >>> from extract_emails.link_filters import DefaultLinkFilter >>> test_urls = [ \"https://example.com/page1.html\" , \"/page.html\" , \"/page.html\" , \"https://google.com\" ] >>> link_filter = DefaultLinkFilter ( \"https://example.com/\" ) >>> filtered_urls = link_filter . filter ( test_urls ) >>> filtered_urls [ \"https://example.com/page1.html\" , \"https://example.com/page.html\" ] Parameters: Name Type Description Default links Iterable[str] List of links for filtering required Returns: Type Description List[str] Set of filtered URLs Source code in extract_emails/link_filters/default_link_filter.py def filter ( self , links : Iterable [ str ]) -> List [ str ]: \"\"\"Will exclude from a list URLs, which not starts with `self.website` and not starts with '/' Examples: >>> from extract_emails.link_filters import DefaultLinkFilter >>> test_urls = [\"https://example.com/page1.html\",\"/page.html\",\"/page.html\", \"https://google.com\"] >>> link_filter = DefaultLinkFilter(\"https://example.com/\") >>> filtered_urls = link_filter.filter(test_urls) >>> filtered_urls [\"https://example.com/page1.html\", \"https://example.com/page.html\"] Args: links: List of links for filtering Returns: Set of filtered URLs \"\"\" filtered_urls = [] for link in links : url = urljoin ( self . website , link ) if not url . startswith ( self . website ): continue if url in self . checked_links : continue filtered_urls . append ( url ) self . checked_links . add ( url ) return filtered_urls ContactInfoLinkFilter \u00b6 Contact information filter for links. Only keep the links might contain the contact information. Examples: >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" ) >>> filtered_links = link_filter . filter ([ '/about-us' , '/search' ]) >>> filtered_links [ 'https://example.com/about-us' ] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , use_default = True ) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [ 'https://example.com/blog' , 'https://example.com/search' ] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , use_default = False ) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , contruct_candidates = [ 'search' ]) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [ 'https://example.com/search' ] __init__ ( self , website , contruct_candidates = None , use_default = False ) special \u00b6 Parameters: Name Type Description Default website str website address (scheme and domain), e.g. https://example.com required contruct_candidates Optional[List[str]] keywords for filtering the list of URLs, default: see self.default_contruct_candidates None use_default bool if no contactinfo urls found and return filtered_urls, default: True False Source code in extract_emails/link_filters/contact_link_filter.py def __init__ ( self , website : str , contruct_candidates : Optional [ List [ str ]] = None , use_default : bool = False , ): \"\"\" Args: website: website address (scheme and domain), e.g. https://example.com contruct_candidates: keywords for filtering the list of URLs, default: see `self.default_contruct_candidates` use_default: if no contactinfo urls found and return filtered_urls, default: True \"\"\" super () . __init__ ( website ) self . checked_links = set () self . candidates = ( contruct_candidates if contruct_candidates is not None else self . default_contruct_candidates ) self . use_default = use_default filter ( self , urls ) \u00b6 Filter out the links without keywords Parameters: Name Type Description Default urls Iterable[str] List of URLs for filtering required Returns: Type Description List[str] List of filtered URLs Source code in extract_emails/link_filters/contact_link_filter.py def filter ( self , urls : Iterable [ str ]) -> List [ str ]: \"\"\"Filter out the links without keywords Args: urls: List of URLs for filtering Returns: List of filtered URLs \"\"\" filtered_urls = [] contactinfo_urls = [] for url in urls : url = urljoin ( self . website , url ) if not url . startswith ( self . website ): continue if url in self . checked_links : continue filtered_urls . append ( url ) self . checked_links . add ( url ) for cand in self . candidates : if cand in url . lower (): contactinfo_urls . append ( url ) break return ( filtered_urls if len ( contactinfo_urls ) == 0 and self . use_default else contactinfo_urls )","title":"Link Filters"},{"location":"code/link_filters/#link-filters","text":"","title":"Link Filters"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase","text":"Base class for link filters","title":"LinkFilterBase"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.__init__","text":"Parameters: Name Type Description Default website str website address (scheme and domain), e.g. https://example.com required Source code in extract_emails/link_filters/link_filter_base.py def __init__ ( self , website : str ): \"\"\" Args: website: website address (scheme and domain), e.g. https://example.com \"\"\" self . website = website","title":"__init__()"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.filter","text":"Filter links by some parameters Parameters: Name Type Description Default urls Iterable[str] List of URLs for filtering required Returns: Type Description List[str] List of filtered URLs Source code in extract_emails/link_filters/link_filter_base.py @abstractmethod def filter ( self , urls : Iterable [ str ]) -> List [ str ]: \"\"\"Filter links by some parameters Args: urls: List of URLs for filtering Returns: List of filtered URLs \"\"\"","title":"filter()"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.get_links","text":"Extract all URLs corresponding to current website Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> links = LinkFilterBase . get_links ( page_source ) >>> links [ \"example.com\" , \"/example.com\" , \"https://example2.com\" ] Parameters: Name Type Description Default page_source str HTML page source required Returns: Type Description List[str] List of URLs :param str page_source: HTML page source :return: List of URLs Source code in extract_emails/link_filters/link_filter_base.py @staticmethod def get_links ( page_source : str ) -> List [ str ]: \"\"\"Extract all URLs corresponding to current website Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> links = LinkFilterBase.get_links(page_source) >>> links [\"example.com\", \"/example.com\", \"https://example2.com\"] Args: page_source: HTML page source Returns: List of URLs :param str page_source: HTML page source :return: List of URLs \"\"\" links = RE_LINKS . findall ( page_source ) links = [ x [ 1 ] for x in links ] return links","title":"get_links()"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.get_website_address","text":"Extract scheme and domain name from an URL Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> website = LinkFilterBase . get_website_address ( 'https://example.com/list?page=134' ) >>> website 'https://example.com/' Parameters: Name Type Description Default url str URL for parsing required Returns: Type Description str scheme and domain name from URL, e.g. https://example.com Source code in extract_emails/link_filters/link_filter_base.py @staticmethod def get_website_address ( url : str ) -> str : \"\"\"Extract scheme and domain name from an URL Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> website = LinkFilterBase.get_website_address('https://example.com/list?page=134') >>> website 'https://example.com/' Args: url: URL for parsing Returns: scheme and domain name from URL, e.g. https://example.com \"\"\" parsed_url = urlparse ( url ) return f \" { parsed_url . scheme } :// { parsed_url . netloc } /\"","title":"get_website_address()"},{"location":"code/link_filters/#extract_emails.link_filters.default_link_filter.DefaultLinkFilter","text":"Default filter for links","title":"DefaultLinkFilter"},{"location":"code/link_filters/#extract_emails.link_filters.default_link_filter.DefaultLinkFilter.filter","text":"Will exclude from a list URLs, which not starts with self.website and not starts with '/' Examples: >>> from extract_emails.link_filters import DefaultLinkFilter >>> test_urls = [ \"https://example.com/page1.html\" , \"/page.html\" , \"/page.html\" , \"https://google.com\" ] >>> link_filter = DefaultLinkFilter ( \"https://example.com/\" ) >>> filtered_urls = link_filter . filter ( test_urls ) >>> filtered_urls [ \"https://example.com/page1.html\" , \"https://example.com/page.html\" ] Parameters: Name Type Description Default links Iterable[str] List of links for filtering required Returns: Type Description List[str] Set of filtered URLs Source code in extract_emails/link_filters/default_link_filter.py def filter ( self , links : Iterable [ str ]) -> List [ str ]: \"\"\"Will exclude from a list URLs, which not starts with `self.website` and not starts with '/' Examples: >>> from extract_emails.link_filters import DefaultLinkFilter >>> test_urls = [\"https://example.com/page1.html\",\"/page.html\",\"/page.html\", \"https://google.com\"] >>> link_filter = DefaultLinkFilter(\"https://example.com/\") >>> filtered_urls = link_filter.filter(test_urls) >>> filtered_urls [\"https://example.com/page1.html\", \"https://example.com/page.html\"] Args: links: List of links for filtering Returns: Set of filtered URLs \"\"\" filtered_urls = [] for link in links : url = urljoin ( self . website , link ) if not url . startswith ( self . website ): continue if url in self . checked_links : continue filtered_urls . append ( url ) self . checked_links . add ( url ) return filtered_urls","title":"filter()"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter","text":"Contact information filter for links. Only keep the links might contain the contact information. Examples: >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" ) >>> filtered_links = link_filter . filter ([ '/about-us' , '/search' ]) >>> filtered_links [ 'https://example.com/about-us' ] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , use_default = True ) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [ 'https://example.com/blog' , 'https://example.com/search' ] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , use_default = False ) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , contruct_candidates = [ 'search' ]) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [ 'https://example.com/search' ]","title":"ContactInfoLinkFilter"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter.__init__","text":"Parameters: Name Type Description Default website str website address (scheme and domain), e.g. https://example.com required contruct_candidates Optional[List[str]] keywords for filtering the list of URLs, default: see self.default_contruct_candidates None use_default bool if no contactinfo urls found and return filtered_urls, default: True False Source code in extract_emails/link_filters/contact_link_filter.py def __init__ ( self , website : str , contruct_candidates : Optional [ List [ str ]] = None , use_default : bool = False , ): \"\"\" Args: website: website address (scheme and domain), e.g. https://example.com contruct_candidates: keywords for filtering the list of URLs, default: see `self.default_contruct_candidates` use_default: if no contactinfo urls found and return filtered_urls, default: True \"\"\" super () . __init__ ( website ) self . checked_links = set () self . candidates = ( contruct_candidates if contruct_candidates is not None else self . default_contruct_candidates ) self . use_default = use_default","title":"__init__()"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter.filter","text":"Filter out the links without keywords Parameters: Name Type Description Default urls Iterable[str] List of URLs for filtering required Returns: Type Description List[str] List of filtered URLs Source code in extract_emails/link_filters/contact_link_filter.py def filter ( self , urls : Iterable [ str ]) -> List [ str ]: \"\"\"Filter out the links without keywords Args: urls: List of URLs for filtering Returns: List of filtered URLs \"\"\" filtered_urls = [] contactinfo_urls = [] for url in urls : url = urljoin ( self . website , url ) if not url . startswith ( self . website ): continue if url in self . checked_links : continue filtered_urls . append ( url ) self . checked_links . add ( url ) for cand in self . candidates : if cand in url . lower (): contactinfo_urls . append ( url ) break return ( filtered_urls if len ( contactinfo_urls ) == 0 and self . use_default else contactinfo_urls )","title":"filter()"},{"location":"code/models/","text":"Models \u00b6 PageData pydantic-model \u00b6 Representation for data from a webpage Examples: >>> from extract_emails.models import PageData >>> page_data = PageData ( website = 'https://example.com' , page_url = 'https://example.com/page123' ) Parameters: Name Type Description Default website str website address from where data required page_url str Page URL from where data required data Optional[Dict[str, List[str]]] Data from the page in format: { 'label': [data, data] }, default: {} required append ( self , label , vals ) \u00b6 Append data from a page to the self.data collection Examples: >>> from extract_emails.models import PageData >>> page_data = PageData ( website = 'https://example.com' , page_url = 'https://example.com/page123' ) >>> page_data . append ( 'email' , [ 'email1@email.com' , 'email2@email.com' ]) >>> page_data . page >>> { 'email' : [ 'email@email.com' , 'email2@email.com' ]} Parameters: Name Type Description Default label str name of collection, e.g. email, linkedin required vals List[str] data from a page, e.g. emails, specific URLs etc. required Source code in extract_emails/models/page_data.py def append ( self , label : str , vals : List [ str ]) -> None : \"\"\"Append data from a page to the self.data collection Examples: >>> from extract_emails.models import PageData >>> page_data = PageData(website='https://example.com', page_url='https://example.com/page123') >>> page_data.append('email', ['email1@email.com', 'email2@email.com']) >>> page_data.page >>> {'email': ['email@email.com', 'email2@email.com']} Args: label: name of collection, e.g. email, linkedin vals: data from a page, e.g. emails, specific URLs etc. \"\"\" try : self . data [ label ] . extend ( vals ) except KeyError : self . data [ label ] = vals","title":"Models"},{"location":"code/models/#models","text":"","title":"Models"},{"location":"code/models/#extract_emails.models.page_data.PageData","text":"Representation for data from a webpage Examples: >>> from extract_emails.models import PageData >>> page_data = PageData ( website = 'https://example.com' , page_url = 'https://example.com/page123' ) Parameters: Name Type Description Default website str website address from where data required page_url str Page URL from where data required data Optional[Dict[str, List[str]]] Data from the page in format: { 'label': [data, data] }, default: {} required","title":"PageData"},{"location":"code/models/#extract_emails.models.page_data.PageData.append","text":"Append data from a page to the self.data collection Examples: >>> from extract_emails.models import PageData >>> page_data = PageData ( website = 'https://example.com' , page_url = 'https://example.com/page123' ) >>> page_data . append ( 'email' , [ 'email1@email.com' , 'email2@email.com' ]) >>> page_data . page >>> { 'email' : [ 'email@email.com' , 'email2@email.com' ]} Parameters: Name Type Description Default label str name of collection, e.g. email, linkedin required vals List[str] data from a page, e.g. emails, specific URLs etc. required Source code in extract_emails/models/page_data.py def append ( self , label : str , vals : List [ str ]) -> None : \"\"\"Append data from a page to the self.data collection Examples: >>> from extract_emails.models import PageData >>> page_data = PageData(website='https://example.com', page_url='https://example.com/page123') >>> page_data.append('email', ['email1@email.com', 'email2@email.com']) >>> page_data.page >>> {'email': ['email@email.com', 'email2@email.com']} Args: label: name of collection, e.g. email, linkedin vals: data from a page, e.g. emails, specific URLs etc. \"\"\" try : self . data [ label ] . extend ( vals ) except KeyError : self . data [ label ] = vals","title":"append()"},{"location":"code/utils/","text":"Utils \u00b6 email_filter ( emails ) \u00b6 Remove duplicated emails and strings looks like emails ( 2@pic.png ) Examples: >>> from extract_emails.utils import email_filter >>> test_emails = [ \"email@email.com\" , \"email@email.com\" , \"2@pic.png\" ] >>> filtered_emails = email_filter ( test_emails ) >>> filtered_emails { \"email@email.com\" } Parameters: Name Type Description Default emails Iterable[str] List of new emails required Returns: Type Description Set[str] List of filtered emails Source code in extract_emails/utils/email_filter.py def email_filter ( emails : Iterable [ str ]) -> Set [ str ]: \"\"\"Remove duplicated emails and strings looks like emails (2@pic.png) Examples: >>> from extract_emails.utils import email_filter >>> test_emails = [\"email@email.com\", \"email@email.com\", \"2@pic.png\"] >>> filtered_emails = email_filter(test_emails) >>> filtered_emails {\"email@email.com\"} Args: emails: List of new emails Returns: List of filtered emails \"\"\" return set ( email for email in emails if \".\" + email . split ( \".\" )[ - 1 ] in TOP_LEVEL_DOMAINS )","title":"Utils"},{"location":"code/utils/#utils","text":"","title":"Utils"},{"location":"code/utils/#extract_emails.utils.email_filter.email_filter","text":"Remove duplicated emails and strings looks like emails ( 2@pic.png ) Examples: >>> from extract_emails.utils import email_filter >>> test_emails = [ \"email@email.com\" , \"email@email.com\" , \"2@pic.png\" ] >>> filtered_emails = email_filter ( test_emails ) >>> filtered_emails { \"email@email.com\" } Parameters: Name Type Description Default emails Iterable[str] List of new emails required Returns: Type Description Set[str] List of filtered emails Source code in extract_emails/utils/email_filter.py def email_filter ( emails : Iterable [ str ]) -> Set [ str ]: \"\"\"Remove duplicated emails and strings looks like emails (2@pic.png) Examples: >>> from extract_emails.utils import email_filter >>> test_emails = [\"email@email.com\", \"email@email.com\", \"2@pic.png\"] >>> filtered_emails = email_filter(test_emails) >>> filtered_emails {\"email@email.com\"} Args: emails: List of new emails Returns: List of filtered emails \"\"\" return set ( email for email in emails if \".\" + email . split ( \".\" )[ - 1 ] in TOP_LEVEL_DOMAINS )","title":"email_filter()"},{"location":"code/workers/","text":"Workers \u00b6 DefaultWorker \u00b6 All data extractions goes here. __init__ ( self , factory ) special \u00b6 Parameters: Name Type Description Default factory BaseFactory see BaseFactory required Source code in extract_emails/workers/default_worker.py def __init__ ( self , factory : BaseFactory ): \"\"\" Args: factory: see `BaseFactory` \"\"\" self . website_url = factory . website_url self . browser = factory . browser self . depth = factory . depth self . max_links_from_page = factory . max_links_from_page self . link_filter = factory . link_filter self . data_extractors = factory . data_extractors self . links = [[ self . website_url ]] self . current_depth = 0 get_data ( self ) \u00b6 Extract data from a given website Source code in extract_emails/workers/default_worker.py def get_data ( self ) -> List [ PageData ]: \"\"\"Extract data from a given website\"\"\" data : List [ PageData ] = [] while len ( self . links ): if self . depth is not None and self . current_depth > self . depth : break self . current_depth += 1 new_data = self . _get_new_data () data . extend ( new_data ) return data","title":"Workers"},{"location":"code/workers/#workers","text":"","title":"Workers"},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker","text":"All data extractions goes here.","title":"DefaultWorker"},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker.__init__","text":"Parameters: Name Type Description Default factory BaseFactory see BaseFactory required Source code in extract_emails/workers/default_worker.py def __init__ ( self , factory : BaseFactory ): \"\"\" Args: factory: see `BaseFactory` \"\"\" self . website_url = factory . website_url self . browser = factory . browser self . depth = factory . depth self . max_links_from_page = factory . max_links_from_page self . link_filter = factory . link_filter self . data_extractors = factory . data_extractors self . links = [[ self . website_url ]] self . current_depth = 0","title":"__init__()"},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker.get_data","text":"Extract data from a given website Source code in extract_emails/workers/default_worker.py def get_data ( self ) -> List [ PageData ]: \"\"\"Extract data from a given website\"\"\" data : List [ PageData ] = [] while len ( self . links ): if self . depth is not None and self . current_depth > self . depth : break self . current_depth += 1 new_data = self . _get_new_data () data . extend ( new_data ) return data","title":"get_data()"},{"location":"quick_start/intro/","text":"Intro \u00b6 There are several main parts in the framework: browser - Class to navigate through specific website and extract data from the webpages ( requests , selenium etc.) link filter - Class to extract URLs from a page corresponding to the website. There are two link filters: DefaultLinkFilter - Will extract all URLs corresponding to the website ContactInfoLinkFilter - Will extract only contact URLs, e.g. /contact/ , /about-us/ etc data extractor - Class to extract data from a page. At the moment there are two data extractors: EmailExtractor - Will extract all emails from the page LinkedinExtractor - Will extract all links to Linkedin profiles from the page factories - Combination of different link filters and data extractors , e.g. DefaultFilterAndEmailFactory or ContactFilterAndEmailAndLinkedinFactory DefaultWorker - All data extractions goes here Simple Usage: \u00b6 from extract_emails.browsers.requests_browser import RequestsBrowser as Browser from extract_emails import DefaultFilterAndEmailFactory as Factory from extract_emails import DefaultWorker browser = Browser () url = 'https://en.wikipedia.org/' factory = Factory ( website_url = url , browser = browser ) worker = DefaultWorker ( factory ) data = worker . get_data () print ( data ) \"\"\" [ PageData( website='https://en.wikipedia.org/', page_url='https://en.wikipedia.org/Email_address', data={'email': ['\"John.Doe.\"@example.com', 'x@example.com']} ), PageData( website='https://en.wikipedia.org/', page_url='https://en.wikipedia.org/Email_address2', data={'email': ['\"John.Doe2.\"@example.com', 'x2@example.com']} ), ] \"\"\"","title":"Quick Start"},{"location":"quick_start/intro/#intro","text":"There are several main parts in the framework: browser - Class to navigate through specific website and extract data from the webpages ( requests , selenium etc.) link filter - Class to extract URLs from a page corresponding to the website. There are two link filters: DefaultLinkFilter - Will extract all URLs corresponding to the website ContactInfoLinkFilter - Will extract only contact URLs, e.g. /contact/ , /about-us/ etc data extractor - Class to extract data from a page. At the moment there are two data extractors: EmailExtractor - Will extract all emails from the page LinkedinExtractor - Will extract all links to Linkedin profiles from the page factories - Combination of different link filters and data extractors , e.g. DefaultFilterAndEmailFactory or ContactFilterAndEmailAndLinkedinFactory DefaultWorker - All data extractions goes here","title":"Intro"},{"location":"quick_start/intro/#simple-usage","text":"from extract_emails.browsers.requests_browser import RequestsBrowser as Browser from extract_emails import DefaultFilterAndEmailFactory as Factory from extract_emails import DefaultWorker browser = Browser () url = 'https://en.wikipedia.org/' factory = Factory ( website_url = url , browser = browser ) worker = DefaultWorker ( factory ) data = worker . get_data () print ( data ) \"\"\" [ PageData( website='https://en.wikipedia.org/', page_url='https://en.wikipedia.org/Email_address', data={'email': ['\"John.Doe.\"@example.com', 'x@example.com']} ), PageData( website='https://en.wikipedia.org/', page_url='https://en.wikipedia.org/Email_address2', data={'email': ['\"John.Doe2.\"@example.com', 'x2@example.com']} ), ] \"\"\"","title":"Simple Usage:"}]}