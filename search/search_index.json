{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Index \u00b6 Quick Start Code References","title":"Extract Emails"},{"location":"#index","text":"Quick Start Code References","title":"Index"},{"location":"changelogs/v5/","text":"V5 \u00b6 5.3.0 \u00b6 Changed \u00b6 Add custom save mode to csv data saver 5.2.0 \u00b6 Added \u00b6 CLI tool csv data saver 5.1.3 \u00b6 Changed \u00b6 Update dependencies 5.1.2 \u00b6 Added \u00b6 Python 3.10 support Add CHANGELOG.md 5.1.0 \u00b6 Added \u00b6 Add save_as_csv class method to PageData model Add logs to DefaultWorker Changed \u00b6 Check if needed libraries for browsers were installed. If not will show user-friendly error Small improvements in the code 5.0.2 \u00b6 Changed \u00b6 Fix imports for factories and DefaultWorker","title":"V5"},{"location":"changelogs/v5/#v5","text":"","title":"V5"},{"location":"changelogs/v5/#530","text":"","title":"5.3.0"},{"location":"changelogs/v5/#changed","text":"Add custom save mode to csv data saver","title":"Changed"},{"location":"changelogs/v5/#520","text":"","title":"5.2.0"},{"location":"changelogs/v5/#added","text":"CLI tool csv data saver","title":"Added"},{"location":"changelogs/v5/#513","text":"","title":"5.1.3"},{"location":"changelogs/v5/#changed_1","text":"Update dependencies","title":"Changed"},{"location":"changelogs/v5/#512","text":"","title":"5.1.2"},{"location":"changelogs/v5/#added_1","text":"Python 3.10 support Add CHANGELOG.md","title":"Added"},{"location":"changelogs/v5/#510","text":"","title":"5.1.0"},{"location":"changelogs/v5/#added_2","text":"Add save_as_csv class method to PageData model Add logs to DefaultWorker","title":"Added"},{"location":"changelogs/v5/#changed_2","text":"Check if needed libraries for browsers were installed. If not will show user-friendly error Small improvements in the code","title":"Changed"},{"location":"changelogs/v5/#502","text":"","title":"5.0.2"},{"location":"changelogs/v5/#changed_3","text":"Fix imports for factories and DefaultWorker","title":"Changed"},{"location":"code/browsers/","text":"Browsers \u00b6 PageSourceGetter \u00b6 All browsers must inherit from this class get_page_source ( self , url ) \u00b6 Return page content from an URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page content (html, json, whatever) Source code in extract_emails/browsers/page_source_getter.py @abstractmethod def get_page_source ( self , url : str ) -> str : \"\"\"Return page content from an URL Args: url: URL Returns: page content (html, json, whatever) \"\"\" ChromeBrowser \u00b6 Getting page sources with selenium and chromedriver Examples: >>> from extract_emails.browsers.chrome_browser import ChromeBrowser >>> browser = ChromeBrowser () >>> browser . open () >>> page_source = browser . get_page_source ( 'https://example.com' ) >>> browser . close () >>> from extract_emails.browsers.chrome_browser import ChromeBrowser >>> with ChromeBrowser () as browser : ... page_source = browser . get_page_source ( 'https://example.com' ) __init__ ( self , executable_path = '/usr/bin/chromedriver' , headless_mode = True , options = None ) special \u00b6 ChromeBrowser initialization Parameters: Name Type Description Default executable_path PathLike path to chromedriver, use which chromedriver to get the path. Default: /usr/bin/chromedriver '/usr/bin/chromedriver' headless_mode bool run browser with headless mode or not. Default: True True options Iterable[str] arguments for chrome.Options(). Default: set(\"--disable-gpu\", \"--disable-software-rasterizer\", \"--disable-dev-shm-usage\", \"--window-size=1920x1080\", \"--disable-setuid-sandbox\", \"--no-sandbox\", ) None Source code in extract_emails/browsers/chrome_browser.py def __init__ ( self , executable_path : PathLike = \"/usr/bin/chromedriver\" , headless_mode : bool = True , options : Iterable [ str ] = None , ) -> None : \"\"\"ChromeBrowser initialization Args: executable_path: path to chromedriver, use `which chromedriver` to get the path. Default: /usr/bin/chromedriver headless_mode: run browser with headless mode or not. Default: True options: arguments for chrome.Options(). Default: set(\"--disable-gpu\", \"--disable-software-rasterizer\", \"--disable-dev-shm-usage\", \"--window-size=1920x1080\", \"--disable-setuid-sandbox\", \"--no-sandbox\", ) \"\"\" self . executable_path = executable_path self . headless_mode = headless_mode self . options = options if options is not None else self . default_options self . driver : Optional [ webdriver . Chrome ] = None close ( self ) \u00b6 Close the browser Source code in extract_emails/browsers/chrome_browser.py def close ( self ): \"\"\"Close the browser\"\"\" self . driver . close () self . driver . quit () get_page_source ( self , url ) \u00b6 Get page source text from URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page source as text Source code in extract_emails/browsers/chrome_browser.py def get_page_source ( self , url : str ) -> str : \"\"\"Get page source text from URL Args: url: URL Returns: page source as text \"\"\" try : self . driver . get ( url ) time . sleep ( self . wait_seconds_after_get ) page_source = self . driver . page_source except Exception as e : logger . error ( f \"Could not get page source from { url } : { e } \" ) return \"\" if \"<html><head></head><body></body></html>\" == page_source : logger . error ( f \"Could not get page source from { url } : Unknown reason\" ) return page_source open ( self ) \u00b6 Add arguments to chrome.Options() and run the browser Source code in extract_emails/browsers/chrome_browser.py def open ( self ): \"\"\"Add arguments to chrome.Options() and run the browser\"\"\" options = Options () for option in self . options : options . add_argument ( option ) if self . headless_mode : options . add_argument ( \"--headless\" ) self . driver = webdriver . Chrome ( options = options , executable_path = self . executable_path ) RequestsBrowser \u00b6 Wrapper on requests library Examples: >>> from extract_emails.browsers.requests_browser import RequestsBrowser >>> browser = RequestsBrowser () >>> page_source = browser . get_page_source ( 'https://example.com' ) __init__ ( self , headers = None ) special \u00b6 Parameters: Name Type Description Default headers Dict[str, Any] headers for requests None Source code in extract_emails/browsers/requests_browser.py def __init__ ( self , headers : Dict [ str , Any ] = None ): \"\"\" Args: headers: headers for requests \"\"\" self . headers = headers self . session = requests . Session () get_page_source ( self , url ) \u00b6 Get page source text from URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page source as text Source code in extract_emails/browsers/requests_browser.py def get_page_source ( self , url : str ) -> str : \"\"\"Get page source text from URL Args: url: URL Returns: page source as text \"\"\" try : response = requests . get ( url , headers = self . headers ) except Exception as e : logger . error ( f \"Could not get page source from { url } : { e } \" ) return \"\" return response . text","title":"Browsers"},{"location":"code/browsers/#browsers","text":"","title":"Browsers"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter","text":"All browsers must inherit from this class","title":"PageSourceGetter"},{"location":"code/browsers/#extract_emails.browsers.page_source_getter.PageSourceGetter.get_page_source","text":"Return page content from an URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page content (html, json, whatever) Source code in extract_emails/browsers/page_source_getter.py @abstractmethod def get_page_source ( self , url : str ) -> str : \"\"\"Return page content from an URL Args: url: URL Returns: page content (html, json, whatever) \"\"\"","title":"get_page_source()"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser","text":"Getting page sources with selenium and chromedriver Examples: >>> from extract_emails.browsers.chrome_browser import ChromeBrowser >>> browser = ChromeBrowser () >>> browser . open () >>> page_source = browser . get_page_source ( 'https://example.com' ) >>> browser . close () >>> from extract_emails.browsers.chrome_browser import ChromeBrowser >>> with ChromeBrowser () as browser : ... page_source = browser . get_page_source ( 'https://example.com' )","title":"ChromeBrowser"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser.__init__","text":"ChromeBrowser initialization Parameters: Name Type Description Default executable_path PathLike path to chromedriver, use which chromedriver to get the path. Default: /usr/bin/chromedriver '/usr/bin/chromedriver' headless_mode bool run browser with headless mode or not. Default: True True options Iterable[str] arguments for chrome.Options(). Default: set(\"--disable-gpu\", \"--disable-software-rasterizer\", \"--disable-dev-shm-usage\", \"--window-size=1920x1080\", \"--disable-setuid-sandbox\", \"--no-sandbox\", ) None Source code in extract_emails/browsers/chrome_browser.py def __init__ ( self , executable_path : PathLike = \"/usr/bin/chromedriver\" , headless_mode : bool = True , options : Iterable [ str ] = None , ) -> None : \"\"\"ChromeBrowser initialization Args: executable_path: path to chromedriver, use `which chromedriver` to get the path. Default: /usr/bin/chromedriver headless_mode: run browser with headless mode or not. Default: True options: arguments for chrome.Options(). Default: set(\"--disable-gpu\", \"--disable-software-rasterizer\", \"--disable-dev-shm-usage\", \"--window-size=1920x1080\", \"--disable-setuid-sandbox\", \"--no-sandbox\", ) \"\"\" self . executable_path = executable_path self . headless_mode = headless_mode self . options = options if options is not None else self . default_options self . driver : Optional [ webdriver . Chrome ] = None","title":"__init__()"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser.close","text":"Close the browser Source code in extract_emails/browsers/chrome_browser.py def close ( self ): \"\"\"Close the browser\"\"\" self . driver . close () self . driver . quit ()","title":"close()"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser.get_page_source","text":"Get page source text from URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page source as text Source code in extract_emails/browsers/chrome_browser.py def get_page_source ( self , url : str ) -> str : \"\"\"Get page source text from URL Args: url: URL Returns: page source as text \"\"\" try : self . driver . get ( url ) time . sleep ( self . wait_seconds_after_get ) page_source = self . driver . page_source except Exception as e : logger . error ( f \"Could not get page source from { url } : { e } \" ) return \"\" if \"<html><head></head><body></body></html>\" == page_source : logger . error ( f \"Could not get page source from { url } : Unknown reason\" ) return page_source","title":"get_page_source()"},{"location":"code/browsers/#extract_emails.browsers.chrome_browser.ChromeBrowser.open","text":"Add arguments to chrome.Options() and run the browser Source code in extract_emails/browsers/chrome_browser.py def open ( self ): \"\"\"Add arguments to chrome.Options() and run the browser\"\"\" options = Options () for option in self . options : options . add_argument ( option ) if self . headless_mode : options . add_argument ( \"--headless\" ) self . driver = webdriver . Chrome ( options = options , executable_path = self . executable_path )","title":"open()"},{"location":"code/browsers/#extract_emails.browsers.requests_browser.RequestsBrowser","text":"Wrapper on requests library Examples: >>> from extract_emails.browsers.requests_browser import RequestsBrowser >>> browser = RequestsBrowser () >>> page_source = browser . get_page_source ( 'https://example.com' )","title":"RequestsBrowser"},{"location":"code/browsers/#extract_emails.browsers.requests_browser.RequestsBrowser.__init__","text":"Parameters: Name Type Description Default headers Dict[str, Any] headers for requests None Source code in extract_emails/browsers/requests_browser.py def __init__ ( self , headers : Dict [ str , Any ] = None ): \"\"\" Args: headers: headers for requests \"\"\" self . headers = headers self . session = requests . Session ()","title":"__init__()"},{"location":"code/browsers/#extract_emails.browsers.requests_browser.RequestsBrowser.get_page_source","text":"Get page source text from URL Parameters: Name Type Description Default url str URL required Returns: Type Description str page source as text Source code in extract_emails/browsers/requests_browser.py def get_page_source ( self , url : str ) -> str : \"\"\"Get page source text from URL Args: url: URL Returns: page source as text \"\"\" try : response = requests . get ( url , headers = self . headers ) except Exception as e : logger . error ( f \"Could not get page source from { url } : { e } \" ) return \"\" return response . text","title":"get_page_source()"},{"location":"code/data_extractors/","text":"Data Extractors \u00b6 DataExtractor \u00b6 Base class for all data extractors name : str property readonly \u00b6 Name of the data extractor, e.g. email, linkedin get_data ( self , page_source ) \u00b6 Extract needed data from a string Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of data, e.g. {' email@email.com ', ' email2@email.com '} Source code in extract_emails/data_extractors/data_extractor.py @abstractmethod def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract needed data from a string Args: page_source: webpage content Returns: Set of data, e.g. {'email@email.com', 'email2@email.com'} \"\"\" EmailExtractor \u00b6 name : str property readonly \u00b6 Name of the data extractor, e.g. email, linkedin get_data ( self , page_source ) \u00b6 Extract emails from a string Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of emails, e.g. {' email@email.com ', ' email2@email.com '} Source code in extract_emails/data_extractors/email_extractor.py def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract emails from a string Args: page_source: webpage content Returns: Set of emails, e.g. {'email@email.com', 'email2@email.com'} \"\"\" raw_emails = [ i for i in self . regexp . findall ( page_source )] return email_filter ( raw_emails ) LinkedinExtractor \u00b6 name : str property readonly \u00b6 Name of the data extractor, e.g. email, linkedin get_data ( self , page_source ) \u00b6 Extract links to Linkedin profiles Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of urls, e.g. {' https://www.linkedin.com/in/venjamin-brant-73381ujy3u '} Source code in extract_emails/data_extractors/linkedin_extractor.py def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract links to Linkedin profiles Args: page_source: webpage content Returns: Set of urls, e.g. {'https://www.linkedin.com/in/venjamin-brant-73381ujy3u'} \"\"\" all_urls = self . regexp . findall ( page_source ) url_filter = \"linkedin.com/in/\" linkedin_urls = set ([ i [ 0 ] for i in all_urls if url_filter in i [ 0 ]]) return linkedin_urls","title":"Data Extractors"},{"location":"code/data_extractors/#data-extractors","text":"","title":"Data Extractors"},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor","text":"Base class for all data extractors","title":"DataExtractor"},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor.name","text":"Name of the data extractor, e.g. email, linkedin","title":"name"},{"location":"code/data_extractors/#extract_emails.data_extractors.data_extractor.DataExtractor.get_data","text":"Extract needed data from a string Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of data, e.g. {' email@email.com ', ' email2@email.com '} Source code in extract_emails/data_extractors/data_extractor.py @abstractmethod def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract needed data from a string Args: page_source: webpage content Returns: Set of data, e.g. {'email@email.com', 'email2@email.com'} \"\"\"","title":"get_data()"},{"location":"code/data_extractors/#extract_emails.data_extractors.email_extractor.EmailExtractor","text":"","title":"EmailExtractor"},{"location":"code/data_extractors/#extract_emails.data_extractors.email_extractor.EmailExtractor.name","text":"Name of the data extractor, e.g. email, linkedin","title":"name"},{"location":"code/data_extractors/#extract_emails.data_extractors.email_extractor.EmailExtractor.get_data","text":"Extract emails from a string Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of emails, e.g. {' email@email.com ', ' email2@email.com '} Source code in extract_emails/data_extractors/email_extractor.py def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract emails from a string Args: page_source: webpage content Returns: Set of emails, e.g. {'email@email.com', 'email2@email.com'} \"\"\" raw_emails = [ i for i in self . regexp . findall ( page_source )] return email_filter ( raw_emails )","title":"get_data()"},{"location":"code/data_extractors/#extract_emails.data_extractors.linkedin_extractor.LinkedinExtractor","text":"","title":"LinkedinExtractor"},{"location":"code/data_extractors/#extract_emails.data_extractors.linkedin_extractor.LinkedinExtractor.name","text":"Name of the data extractor, e.g. email, linkedin","title":"name"},{"location":"code/data_extractors/#extract_emails.data_extractors.linkedin_extractor.LinkedinExtractor.get_data","text":"Extract links to Linkedin profiles Parameters: Name Type Description Default page_source str webpage content required Returns: Type Description Set[str] Set of urls, e.g. {' https://www.linkedin.com/in/venjamin-brant-73381ujy3u '} Source code in extract_emails/data_extractors/linkedin_extractor.py def get_data ( self , page_source : str ) -> Set [ str ]: \"\"\"Extract links to Linkedin profiles Args: page_source: webpage content Returns: Set of urls, e.g. {'https://www.linkedin.com/in/venjamin-brant-73381ujy3u'} \"\"\" all_urls = self . regexp . findall ( page_source ) url_filter = \"linkedin.com/in/\" linkedin_urls = set ([ i [ 0 ] for i in all_urls if url_filter in i [ 0 ]]) return linkedin_urls","title":"get_data()"},{"location":"code/errors/","text":"Errors \u00b6 BrowserImportError \u00b6 Error for cases when required libraries for browsers were not installed","title":"Errors"},{"location":"code/errors/#errors","text":"","title":"Errors"},{"location":"code/errors/#extract_emails.errors.errors.BrowserImportError","text":"Error for cases when required libraries for browsers were not installed","title":"BrowserImportError"},{"location":"code/factories/","text":"Factories \u00b6 BaseFactory \u00b6 Base class for all factories data_extractors : List [ Type [ extract_emails . data_extractors . data_extractor . DataExtractor ]] property readonly \u00b6 Initialize data extractors link_filter : Type [ extract_emails . link_filters . link_filter_base . LinkFilterBase ] property readonly \u00b6 Initialize link filter __init__ ( self , * , website_url , browser , depth = None , max_links_from_page = None ) special \u00b6 Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10 None max_links_from_page Optional[int] how many links a script shall get from each page, default None (all) None Source code in extract_emails/factories/base_factory.py def __init__ ( self , * , website_url : str , browser : PageSourceGetter , depth : Optional [ int ] = None , max_links_from_page : Optional [ int ] = None , ): \"\"\" Args: website_url: website for scan, e.g. https://example.com browser: browser to get page source by URL depth: scan's depth, default 10 max_links_from_page: how many links a script shall get from each page, default None (all) \"\"\" self . _website_url = website_url self . _browser = browser self . _depth = depth self . _max_links_from_page = max_links_from_page DefaultFilterAndEmailFactory \u00b6 Will initialize DefaultLinkFilter and EmailExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndEmailFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe2.\"@example.com' , 'x2@example.com' ]} ), ] data_extractors : List [ extract_emails . data_extractors . email_extractor . EmailExtractor ] property readonly \u00b6 Initialize EmailExtractor link_filter : DefaultLinkFilter property readonly \u00b6 Initialize DefaultLinkFilter DefaultFilterAndLinkedinFactory \u00b6 Will initialize DefaultLinkFilter and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'linkedin' : [ 'linkeding profile url 1' , 'linkeding profile url 2' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'linkedin' : [ 'linkeding profile url 3' , 'linkeding profile url 4' ]} ), ] data_extractors : List [ extract_emails . data_extractors . linkedin_extractor . LinkedinExtractor ] property readonly \u00b6 Initialize LinkedinExtractor link_filter : DefaultLinkFilter property readonly \u00b6 Initialize DefaultLinkFilter DefaultFilterAndEmailAndLinkedinFactory \u00b6 Will initialize DefaultLinkFilter and EmailExtractor and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndEmailAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url1' , 'linkedin_url2' ], } ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url3' , 'linkedin_url4' ], } ), ] data_extractors : List [ Union [ extract_emails . data_extractors . email_extractor . EmailExtractor , extract_emails . data_extractors . linkedin_extractor . LinkedinExtractor ]] property readonly \u00b6 Initialize EmailExtractor and LinkedinExtractor link_filter : DefaultLinkFilter property readonly \u00b6 Initialize DefaultLinkFilter ContactFilterAndEmailFactory \u00b6 Will initialize ContactInfoLinkFilter and EmailExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndEmailFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe2.\"@example.com' , 'x2@example.com' ]} ), ] data_extractors : List [ extract_emails . data_extractors . email_extractor . EmailExtractor ] property readonly \u00b6 Initialize EmailExtractor link_filter : ContactInfoLinkFilter property readonly \u00b6 Initialize ContactInfoLinkFilter ContactFilterAndLinkedinFactory \u00b6 Will initialize ContactInfoLinkFilter and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'linkedin' : [ 'linkeding profile url 1' , 'linkeding profile url 2' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'linkedin' : [ 'linkeding profile url 3' , 'linkeding profile url 4' ]} ), ] data_extractors : List [ extract_emails . data_extractors . linkedin_extractor . LinkedinExtractor ] property readonly \u00b6 Initialize LinkedinExtractor link_filter : ContactInfoLinkFilter property readonly \u00b6 Initialize ContactInfoLinkFilter ContactFilterAndEmailAndLinkedinFactory \u00b6 Will initialize ContactInfoLinkFilter and EmailExtractor and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndEmailAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url1' , 'linkedin_url2' ], } ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url3' , 'linkedin_url4' ], } ), ] data_extractors : List [ Union [ extract_emails . data_extractors . email_extractor . EmailExtractor , extract_emails . data_extractors . linkedin_extractor . LinkedinExtractor ]] property readonly \u00b6 Initialize EmailExtractor and LinkedinExtractor link_filter : ContactInfoLinkFilter property readonly \u00b6 Initialize DefaultLinkFilter","title":"Factories"},{"location":"code/factories/#factories","text":"","title":"Factories"},{"location":"code/factories/#extract_emails.factories.base_factory.BaseFactory","text":"Base class for all factories","title":"BaseFactory"},{"location":"code/factories/#extract_emails.factories.base_factory.BaseFactory.data_extractors","text":"Initialize data extractors","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.base_factory.BaseFactory.link_filter","text":"Initialize link filter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.base_factory.BaseFactory.__init__","text":"Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10 None max_links_from_page Optional[int] how many links a script shall get from each page, default None (all) None Source code in extract_emails/factories/base_factory.py def __init__ ( self , * , website_url : str , browser : PageSourceGetter , depth : Optional [ int ] = None , max_links_from_page : Optional [ int ] = None , ): \"\"\" Args: website_url: website for scan, e.g. https://example.com browser: browser to get page source by URL depth: scan's depth, default 10 max_links_from_page: how many links a script shall get from each page, default None (all) \"\"\" self . _website_url = website_url self . _browser = browser self . _depth = depth self . _max_links_from_page = max_links_from_page","title":"__init__()"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email.DefaultFilterAndEmailFactory","text":"Will initialize DefaultLinkFilter and EmailExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndEmailFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe2.\"@example.com' , 'x2@example.com' ]} ), ]","title":"DefaultFilterAndEmailFactory"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email.DefaultFilterAndEmailFactory.data_extractors","text":"Initialize EmailExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email.DefaultFilterAndEmailFactory.link_filter","text":"Initialize DefaultLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.default_filter_and_linkedin.DefaultFilterAndLinkedinFactory","text":"Will initialize DefaultLinkFilter and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'linkedin' : [ 'linkeding profile url 1' , 'linkeding profile url 2' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'linkedin' : [ 'linkeding profile url 3' , 'linkeding profile url 4' ]} ), ]","title":"DefaultFilterAndLinkedinFactory"},{"location":"code/factories/#extract_emails.factories.default_filter_and_linkedin.DefaultFilterAndLinkedinFactory.data_extractors","text":"Initialize LinkedinExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.default_filter_and_linkedin.DefaultFilterAndLinkedinFactory.link_filter","text":"Initialize DefaultLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email_and_linkedin.DefaultFilterAndEmailAndLinkedinFactory","text":"Will initialize DefaultLinkFilter and EmailExtractor and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import DefaultFilterAndEmailAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url1' , 'linkedin_url2' ], } ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url3' , 'linkedin_url4' ], } ), ]","title":"DefaultFilterAndEmailAndLinkedinFactory"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email_and_linkedin.DefaultFilterAndEmailAndLinkedinFactory.data_extractors","text":"Initialize EmailExtractor and LinkedinExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.default_filter_and_email_and_linkedin.DefaultFilterAndEmailAndLinkedinFactory.link_filter","text":"Initialize DefaultLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email.ContactFilterAndEmailFactory","text":"Will initialize ContactInfoLinkFilter and EmailExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndEmailFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe2.\"@example.com' , 'x2@example.com' ]} ), ]","title":"ContactFilterAndEmailFactory"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email.ContactFilterAndEmailFactory.data_extractors","text":"Initialize EmailExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email.ContactFilterAndEmailFactory.link_filter","text":"Initialize ContactInfoLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_linkedin.ContactFilterAndLinkedinFactory","text":"Will initialize ContactInfoLinkFilter and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'linkedin' : [ 'linkeding profile url 1' , 'linkeding profile url 2' ]} ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'linkedin' : [ 'linkeding profile url 3' , 'linkeding profile url 4' ]} ), ]","title":"ContactFilterAndLinkedinFactory"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_linkedin.ContactFilterAndLinkedinFactory.data_extractors","text":"Initialize LinkedinExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_linkedin.ContactFilterAndLinkedinFactory.link_filter","text":"Initialize ContactInfoLinkFilter","title":"link_filter"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email_and_linkedin.ContactFilterAndEmailAndLinkedinFactory","text":"Will initialize ContactInfoLinkFilter and EmailExtractor and LinkedinExtractor Parameters: Name Type Description Default website_url str website for scan, e.g. https://example.com required browser PageSourceGetter browser to get page source by URL required depth Optional[int] scan's depth, default 10. Defaults to None required max_links_from_page Optional[int] how many links a script shall get from each page. Defaults to None required Examples: >>> from extract_emails import ContactFilterAndEmailAndLinkedinFactory as Factory >>> from extract_emails import DefaultWorker >>> from extract_emails.browsers.requests_browser import RequestsBrowser as Browser >>> >>> browser = Browser () >>> url = 'https://en.wikipedia.org/' >>> factory = Factory ( website_url = url , browser = browser ) >>> worker = DefaultWorker ( factory ) >>> data = worker . get_data () >>> data [ PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url1' , 'linkedin_url2' ], } ), PageData ( website = 'https://en.wikipedia.org/' , page_url = 'https://en.wikipedia.org/Email_address2' , data = { 'email' : [ '\"John.Doe.\"@example.com' , 'x@example.com' ], 'linkedin' : [ 'linkedin_url3' , 'linkedin_url4' ], } ), ]","title":"ContactFilterAndEmailAndLinkedinFactory"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email_and_linkedin.ContactFilterAndEmailAndLinkedinFactory.data_extractors","text":"Initialize EmailExtractor and LinkedinExtractor","title":"data_extractors"},{"location":"code/factories/#extract_emails.factories.contact_filter_and_email_and_linkedin.ContactFilterAndEmailAndLinkedinFactory.link_filter","text":"Initialize DefaultLinkFilter","title":"link_filter"},{"location":"code/link_filters/","text":"Link Filters \u00b6 LinkFilterBase \u00b6 Base class for link filters __init__ ( self , website ) special \u00b6 Parameters: Name Type Description Default website str website address (scheme and domain), e.g. https://example.com required Source code in extract_emails/link_filters/link_filter_base.py def __init__ ( self , website : str ): \"\"\" Args: website: website address (scheme and domain), e.g. https://example.com \"\"\" self . website = website filter ( self , urls ) \u00b6 Filter links by some parameters Parameters: Name Type Description Default urls Iterable[str] List of URLs for filtering required Returns: Type Description List[str] List of filtered URLs Source code in extract_emails/link_filters/link_filter_base.py @abstractmethod def filter ( self , urls : Iterable [ str ]) -> List [ str ]: \"\"\"Filter links by some parameters Args: urls: List of URLs for filtering Returns: List of filtered URLs \"\"\" get_links ( page_source ) staticmethod \u00b6 Extract all URLs corresponding to current website Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> links = LinkFilterBase . get_links ( page_source ) >>> links [ \"example.com\" , \"/example.com\" , \"https://example2.com\" ] Parameters: Name Type Description Default page_source str HTML page source required Returns: Type Description List[str] List of URLs :param str page_source: HTML page source :return: List of URLs Source code in extract_emails/link_filters/link_filter_base.py @staticmethod def get_links ( page_source : str ) -> List [ str ]: \"\"\"Extract all URLs corresponding to current website Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> links = LinkFilterBase.get_links(page_source) >>> links [\"example.com\", \"/example.com\", \"https://example2.com\"] Args: page_source: HTML page source Returns: List of URLs :param str page_source: HTML page source :return: List of URLs \"\"\" links = RE_LINKS . findall ( page_source ) links = [ x [ 1 ] for x in links ] return links get_website_address ( url ) staticmethod \u00b6 Extract scheme and domain name from an URL Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> website = LinkFilterBase . get_website_address ( 'https://example.com/list?page=134' ) >>> website 'https://example.com/' Parameters: Name Type Description Default url str URL for parsing required Returns: Type Description str scheme and domain name from URL, e.g. https://example.com Source code in extract_emails/link_filters/link_filter_base.py @staticmethod def get_website_address ( url : str ) -> str : \"\"\"Extract scheme and domain name from an URL Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> website = LinkFilterBase.get_website_address('https://example.com/list?page=134') >>> website 'https://example.com/' Args: url: URL for parsing Returns: scheme and domain name from URL, e.g. https://example.com \"\"\" parsed_url = urlparse ( url ) return f \" { parsed_url . scheme } :// { parsed_url . netloc } /\" DefaultLinkFilter \u00b6 Default filter for links filter ( self , links ) \u00b6 Will exclude from a list URLs, which not starts with self.website and not starts with '/' Examples: >>> from extract_emails.link_filters import DefaultLinkFilter >>> test_urls = [ \"https://example.com/page1.html\" , \"/page.html\" , \"/page.html\" , \"https://google.com\" ] >>> link_filter = DefaultLinkFilter ( \"https://example.com/\" ) >>> filtered_urls = link_filter . filter ( test_urls ) >>> filtered_urls [ \"https://example.com/page1.html\" , \"https://example.com/page.html\" ] Parameters: Name Type Description Default links Iterable[str] List of links for filtering required Returns: Type Description List[str] Set of filtered URLs Source code in extract_emails/link_filters/default_link_filter.py def filter ( self , links : Iterable [ str ]) -> List [ str ]: \"\"\"Will exclude from a list URLs, which not starts with `self.website` and not starts with '/' Examples: >>> from extract_emails.link_filters import DefaultLinkFilter >>> test_urls = [\"https://example.com/page1.html\",\"/page.html\",\"/page.html\", \"https://google.com\"] >>> link_filter = DefaultLinkFilter(\"https://example.com/\") >>> filtered_urls = link_filter.filter(test_urls) >>> filtered_urls [\"https://example.com/page1.html\", \"https://example.com/page.html\"] Args: links: List of links for filtering Returns: Set of filtered URLs \"\"\" filtered_urls = [] for link in links : url = urljoin ( self . website , link ) if not url . startswith ( self . website ): continue if url in self . checked_links : continue filtered_urls . append ( url ) self . checked_links . add ( url ) return filtered_urls ContactInfoLinkFilter \u00b6 Contact information filter for links. Only keep the links might contain the contact information. Examples: >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" ) >>> filtered_links = link_filter . filter ([ '/about-us' , '/search' ]) >>> filtered_links [ 'https://example.com/about-us' ] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , use_default = True ) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [ 'https://example.com/blog' , 'https://example.com/search' ] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , use_default = False ) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , contruct_candidates = [ 'search' ]) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [ 'https://example.com/search' ] __init__ ( self , website , contruct_candidates = None , use_default = False ) special \u00b6 Parameters: Name Type Description Default website str website address (scheme and domain), e.g. https://example.com required contruct_candidates Optional[List[str]] keywords for filtering the list of URLs, default: see self.default_contruct_candidates None use_default bool if no contactinfo urls found and return filtered_urls, default: True False Source code in extract_emails/link_filters/contact_link_filter.py def __init__ ( self , website : str , contruct_candidates : Optional [ List [ str ]] = None , use_default : bool = False , ): \"\"\" Args: website: website address (scheme and domain), e.g. https://example.com contruct_candidates: keywords for filtering the list of URLs, default: see `self.default_contruct_candidates` use_default: if no contactinfo urls found and return filtered_urls, default: True \"\"\" super () . __init__ ( website ) self . checked_links = set () self . candidates = ( contruct_candidates if contruct_candidates is not None else self . default_contruct_candidates ) self . use_default = use_default filter ( self , urls ) \u00b6 Filter out the links without keywords Parameters: Name Type Description Default urls Iterable[str] List of URLs for filtering required Returns: Type Description List[str] List of filtered URLs Source code in extract_emails/link_filters/contact_link_filter.py def filter ( self , urls : Iterable [ str ]) -> List [ str ]: \"\"\"Filter out the links without keywords Args: urls: List of URLs for filtering Returns: List of filtered URLs \"\"\" filtered_urls = [] contactinfo_urls = [] for url in urls : url = urljoin ( self . website , url ) if not url . startswith ( self . website ): continue if url in self . checked_links : continue filtered_urls . append ( url ) self . checked_links . add ( url ) for cand in self . candidates : if cand in url . lower (): contactinfo_urls . append ( url ) break return ( filtered_urls if len ( contactinfo_urls ) == 0 and self . use_default else contactinfo_urls )","title":"Link Filters"},{"location":"code/link_filters/#link-filters","text":"","title":"Link Filters"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase","text":"Base class for link filters","title":"LinkFilterBase"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.__init__","text":"Parameters: Name Type Description Default website str website address (scheme and domain), e.g. https://example.com required Source code in extract_emails/link_filters/link_filter_base.py def __init__ ( self , website : str ): \"\"\" Args: website: website address (scheme and domain), e.g. https://example.com \"\"\" self . website = website","title":"__init__()"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.filter","text":"Filter links by some parameters Parameters: Name Type Description Default urls Iterable[str] List of URLs for filtering required Returns: Type Description List[str] List of filtered URLs Source code in extract_emails/link_filters/link_filter_base.py @abstractmethod def filter ( self , urls : Iterable [ str ]) -> List [ str ]: \"\"\"Filter links by some parameters Args: urls: List of URLs for filtering Returns: List of filtered URLs \"\"\"","title":"filter()"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.get_links","text":"Extract all URLs corresponding to current website Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> links = LinkFilterBase . get_links ( page_source ) >>> links [ \"example.com\" , \"/example.com\" , \"https://example2.com\" ] Parameters: Name Type Description Default page_source str HTML page source required Returns: Type Description List[str] List of URLs :param str page_source: HTML page source :return: List of URLs Source code in extract_emails/link_filters/link_filter_base.py @staticmethod def get_links ( page_source : str ) -> List [ str ]: \"\"\"Extract all URLs corresponding to current website Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> links = LinkFilterBase.get_links(page_source) >>> links [\"example.com\", \"/example.com\", \"https://example2.com\"] Args: page_source: HTML page source Returns: List of URLs :param str page_source: HTML page source :return: List of URLs \"\"\" links = RE_LINKS . findall ( page_source ) links = [ x [ 1 ] for x in links ] return links","title":"get_links()"},{"location":"code/link_filters/#extract_emails.link_filters.link_filter_base.LinkFilterBase.get_website_address","text":"Extract scheme and domain name from an URL Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> website = LinkFilterBase . get_website_address ( 'https://example.com/list?page=134' ) >>> website 'https://example.com/' Parameters: Name Type Description Default url str URL for parsing required Returns: Type Description str scheme and domain name from URL, e.g. https://example.com Source code in extract_emails/link_filters/link_filter_base.py @staticmethod def get_website_address ( url : str ) -> str : \"\"\"Extract scheme and domain name from an URL Examples: >>> from extract_emails.link_filters import LinkFilterBase >>> website = LinkFilterBase.get_website_address('https://example.com/list?page=134') >>> website 'https://example.com/' Args: url: URL for parsing Returns: scheme and domain name from URL, e.g. https://example.com \"\"\" parsed_url = urlparse ( url ) return f \" { parsed_url . scheme } :// { parsed_url . netloc } /\"","title":"get_website_address()"},{"location":"code/link_filters/#extract_emails.link_filters.default_link_filter.DefaultLinkFilter","text":"Default filter for links","title":"DefaultLinkFilter"},{"location":"code/link_filters/#extract_emails.link_filters.default_link_filter.DefaultLinkFilter.filter","text":"Will exclude from a list URLs, which not starts with self.website and not starts with '/' Examples: >>> from extract_emails.link_filters import DefaultLinkFilter >>> test_urls = [ \"https://example.com/page1.html\" , \"/page.html\" , \"/page.html\" , \"https://google.com\" ] >>> link_filter = DefaultLinkFilter ( \"https://example.com/\" ) >>> filtered_urls = link_filter . filter ( test_urls ) >>> filtered_urls [ \"https://example.com/page1.html\" , \"https://example.com/page.html\" ] Parameters: Name Type Description Default links Iterable[str] List of links for filtering required Returns: Type Description List[str] Set of filtered URLs Source code in extract_emails/link_filters/default_link_filter.py def filter ( self , links : Iterable [ str ]) -> List [ str ]: \"\"\"Will exclude from a list URLs, which not starts with `self.website` and not starts with '/' Examples: >>> from extract_emails.link_filters import DefaultLinkFilter >>> test_urls = [\"https://example.com/page1.html\",\"/page.html\",\"/page.html\", \"https://google.com\"] >>> link_filter = DefaultLinkFilter(\"https://example.com/\") >>> filtered_urls = link_filter.filter(test_urls) >>> filtered_urls [\"https://example.com/page1.html\", \"https://example.com/page.html\"] Args: links: List of links for filtering Returns: Set of filtered URLs \"\"\" filtered_urls = [] for link in links : url = urljoin ( self . website , link ) if not url . startswith ( self . website ): continue if url in self . checked_links : continue filtered_urls . append ( url ) self . checked_links . add ( url ) return filtered_urls","title":"filter()"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter","text":"Contact information filter for links. Only keep the links might contain the contact information. Examples: >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" ) >>> filtered_links = link_filter . filter ([ '/about-us' , '/search' ]) >>> filtered_links [ 'https://example.com/about-us' ] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , use_default = True ) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [ 'https://example.com/blog' , 'https://example.com/search' ] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , use_default = False ) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [] >>> from extract_emails.link_filters import ContactInfoLinkFilter >>> link_filter = ContactInfoLinkFilter ( \"https://example.com\" , contruct_candidates = [ 'search' ]) >>> filtered_links = link_filter . filter ([ '/blog' , '/search' ]) >>> filtered_links [ 'https://example.com/search' ]","title":"ContactInfoLinkFilter"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter.__init__","text":"Parameters: Name Type Description Default website str website address (scheme and domain), e.g. https://example.com required contruct_candidates Optional[List[str]] keywords for filtering the list of URLs, default: see self.default_contruct_candidates None use_default bool if no contactinfo urls found and return filtered_urls, default: True False Source code in extract_emails/link_filters/contact_link_filter.py def __init__ ( self , website : str , contruct_candidates : Optional [ List [ str ]] = None , use_default : bool = False , ): \"\"\" Args: website: website address (scheme and domain), e.g. https://example.com contruct_candidates: keywords for filtering the list of URLs, default: see `self.default_contruct_candidates` use_default: if no contactinfo urls found and return filtered_urls, default: True \"\"\" super () . __init__ ( website ) self . checked_links = set () self . candidates = ( contruct_candidates if contruct_candidates is not None else self . default_contruct_candidates ) self . use_default = use_default","title":"__init__()"},{"location":"code/link_filters/#extract_emails.link_filters.contact_link_filter.ContactInfoLinkFilter.filter","text":"Filter out the links without keywords Parameters: Name Type Description Default urls Iterable[str] List of URLs for filtering required Returns: Type Description List[str] List of filtered URLs Source code in extract_emails/link_filters/contact_link_filter.py def filter ( self , urls : Iterable [ str ]) -> List [ str ]: \"\"\"Filter out the links without keywords Args: urls: List of URLs for filtering Returns: List of filtered URLs \"\"\" filtered_urls = [] contactinfo_urls = [] for url in urls : url = urljoin ( self . website , url ) if not url . startswith ( self . website ): continue if url in self . checked_links : continue filtered_urls . append ( url ) self . checked_links . add ( url ) for cand in self . candidates : if cand in url . lower (): contactinfo_urls . append ( url ) break return ( filtered_urls if len ( contactinfo_urls ) == 0 and self . use_default else contactinfo_urls )","title":"filter()"},{"location":"code/models/","text":"Models \u00b6 PageData pydantic-model \u00b6 Representation for data from a webpage Examples: >>> from extract_emails.models import PageData >>> page_data = PageData ( website = 'https://example.com' , page_url = 'https://example.com/page123' ) Attributes: Name Type Description website str website address from where data page_url str Page URL from where data data Optional[Dict[str, List[str]]] Data from the page in format: { 'label': [data, data] }, default: {} append ( self , label , vals ) \u00b6 Append data from a page to the self.data collection Examples: >>> from extract_emails.models import PageData >>> page_data = PageData ( website = 'https://example.com' , page_url = 'https://example.com/page123' ) >>> page_data . append ( 'email' , [ 'email1@email.com' , 'email2@email.com' ]) >>> page_data . page >>> { 'email' : [ 'email@email.com' , 'email2@email.com' ]} Parameters: Name Type Description Default label str name of collection, e.g. email, linkedin required vals List[str] data from a page, e.g. emails, specific URLs etc. required Source code in extract_emails/models/page_data.py def append ( self , label : str , vals : List [ str ]) -> None : \"\"\"Append data from a page to the self.data collection Examples: >>> from extract_emails.models import PageData >>> page_data = PageData(website='https://example.com', page_url='https://example.com/page123') >>> page_data.append('email', ['email1@email.com', 'email2@email.com']) >>> page_data.page >>> {'email': ['email@email.com', 'email2@email.com']} Args: label: name of collection, e.g. email, linkedin vals: data from a page, e.g. emails, specific URLs etc. \"\"\" try : self . data [ label ] . extend ( vals ) except KeyError : self . data [ label ] = vals save_as_csv ( data , filepath ) classmethod \u00b6 Save list of PageData to CSV file Parameters: Name Type Description Default data List[PageData] list of PageData required filepath PathLike path to a CSV file required Source code in extract_emails/models/page_data.py @classmethod def save_as_csv ( cls , data : List [ \"PageData\" ], filepath : PathLike ) -> None : \"\"\"Save list of `PageData` to CSV file Args: data: list of `PageData` filepath: path to a CSV file \"\"\" base_headers : List [ str ] = list ( cls . schema ()[ \"properties\" ] . keys ()) base_headers . remove ( \"data\" ) data_headers = [ i for i in data [ 0 ] . data . keys ()] headers = base_headers + data_headers with open ( filepath , \"w\" , encoding = \"utf-8\" , newline = \"\" ) as f : writer = csv . DictWriter ( f , fieldnames = headers ) writer . writeheader () for page in data : for data_in_row in zip_longest ( * page . data . values ()): new_row = { \"website\" : page . website , \"page_url\" : page . page_url } for counter , column in enumerate ( data_headers ): new_row [ column ] = data_in_row [ counter ] writer . writerow ( new_row )","title":"Models"},{"location":"code/models/#models","text":"","title":"Models"},{"location":"code/models/#extract_emails.models.page_data.PageData","text":"Representation for data from a webpage Examples: >>> from extract_emails.models import PageData >>> page_data = PageData ( website = 'https://example.com' , page_url = 'https://example.com/page123' ) Attributes: Name Type Description website str website address from where data page_url str Page URL from where data data Optional[Dict[str, List[str]]] Data from the page in format: { 'label': [data, data] }, default: {}","title":"PageData"},{"location":"code/models/#extract_emails.models.page_data.PageData.append","text":"Append data from a page to the self.data collection Examples: >>> from extract_emails.models import PageData >>> page_data = PageData ( website = 'https://example.com' , page_url = 'https://example.com/page123' ) >>> page_data . append ( 'email' , [ 'email1@email.com' , 'email2@email.com' ]) >>> page_data . page >>> { 'email' : [ 'email@email.com' , 'email2@email.com' ]} Parameters: Name Type Description Default label str name of collection, e.g. email, linkedin required vals List[str] data from a page, e.g. emails, specific URLs etc. required Source code in extract_emails/models/page_data.py def append ( self , label : str , vals : List [ str ]) -> None : \"\"\"Append data from a page to the self.data collection Examples: >>> from extract_emails.models import PageData >>> page_data = PageData(website='https://example.com', page_url='https://example.com/page123') >>> page_data.append('email', ['email1@email.com', 'email2@email.com']) >>> page_data.page >>> {'email': ['email@email.com', 'email2@email.com']} Args: label: name of collection, e.g. email, linkedin vals: data from a page, e.g. emails, specific URLs etc. \"\"\" try : self . data [ label ] . extend ( vals ) except KeyError : self . data [ label ] = vals","title":"append()"},{"location":"code/models/#extract_emails.models.page_data.PageData.save_as_csv","text":"Save list of PageData to CSV file Parameters: Name Type Description Default data List[PageData] list of PageData required filepath PathLike path to a CSV file required Source code in extract_emails/models/page_data.py @classmethod def save_as_csv ( cls , data : List [ \"PageData\" ], filepath : PathLike ) -> None : \"\"\"Save list of `PageData` to CSV file Args: data: list of `PageData` filepath: path to a CSV file \"\"\" base_headers : List [ str ] = list ( cls . schema ()[ \"properties\" ] . keys ()) base_headers . remove ( \"data\" ) data_headers = [ i for i in data [ 0 ] . data . keys ()] headers = base_headers + data_headers with open ( filepath , \"w\" , encoding = \"utf-8\" , newline = \"\" ) as f : writer = csv . DictWriter ( f , fieldnames = headers ) writer . writeheader () for page in data : for data_in_row in zip_longest ( * page . data . values ()): new_row = { \"website\" : page . website , \"page_url\" : page . page_url } for counter , column in enumerate ( data_headers ): new_row [ column ] = data_in_row [ counter ] writer . writerow ( new_row )","title":"save_as_csv()"},{"location":"code/utils/","text":"Utils \u00b6 email_filter ( emails ) \u00b6 Remove duplicated emails and strings looks like emails ( 2@pic.png ) Examples: >>> from extract_emails.utils import email_filter >>> test_emails = [ \"email@email.com\" , \"email@email.com\" , \"2@pic.png\" ] >>> filtered_emails = email_filter ( test_emails ) >>> filtered_emails { \"email@email.com\" } Parameters: Name Type Description Default emails Iterable[str] List of new emails required Returns: Type Description Set[str] List of filtered emails Source code in extract_emails/utils/email_filter.py def email_filter ( emails : Iterable [ str ]) -> Set [ str ]: \"\"\"Remove duplicated emails and strings looks like emails (2@pic.png) Examples: >>> from extract_emails.utils import email_filter >>> test_emails = [\"email@email.com\", \"email@email.com\", \"2@pic.png\"] >>> filtered_emails = email_filter(test_emails) >>> filtered_emails {\"email@email.com\"} Args: emails: List of new emails Returns: List of filtered emails \"\"\" return set ( email for email in emails if \".\" + email . split ( \".\" )[ - 1 ] in TOP_LEVEL_DOMAINS )","title":"Utils"},{"location":"code/utils/#utils","text":"","title":"Utils"},{"location":"code/utils/#extract_emails.utils.email_filter.email_filter","text":"Remove duplicated emails and strings looks like emails ( 2@pic.png ) Examples: >>> from extract_emails.utils import email_filter >>> test_emails = [ \"email@email.com\" , \"email@email.com\" , \"2@pic.png\" ] >>> filtered_emails = email_filter ( test_emails ) >>> filtered_emails { \"email@email.com\" } Parameters: Name Type Description Default emails Iterable[str] List of new emails required Returns: Type Description Set[str] List of filtered emails Source code in extract_emails/utils/email_filter.py def email_filter ( emails : Iterable [ str ]) -> Set [ str ]: \"\"\"Remove duplicated emails and strings looks like emails (2@pic.png) Examples: >>> from extract_emails.utils import email_filter >>> test_emails = [\"email@email.com\", \"email@email.com\", \"2@pic.png\"] >>> filtered_emails = email_filter(test_emails) >>> filtered_emails {\"email@email.com\"} Args: emails: List of new emails Returns: List of filtered emails \"\"\" return set ( email for email in emails if \".\" + email . split ( \".\" )[ - 1 ] in TOP_LEVEL_DOMAINS )","title":"email_filter()"},{"location":"code/workers/","text":"Workers \u00b6 DefaultWorker \u00b6 All data extractions goes here. __init__ ( self , factory ) special \u00b6 Parameters: Name Type Description Default factory BaseFactory see BaseFactory required Source code in extract_emails/workers/default_worker.py def __init__ ( self , factory : BaseFactory ): \"\"\" Args: factory: see `BaseFactory` \"\"\" self . website_url = factory . website_url self . browser = factory . browser self . depth = factory . depth self . max_links_from_page = factory . max_links_from_page self . link_filter = factory . link_filter self . data_extractors = factory . data_extractors self . links = [[ self . website_url ]] self . current_depth = 0 get_data ( self ) \u00b6 Extract data from a given website Source code in extract_emails/workers/default_worker.py def get_data ( self ) -> List [ PageData ]: \"\"\"Extract data from a given website\"\"\" data : List [ PageData ] = [] while len ( self . links ): logger . debug ( f \"current_depth= { self . current_depth } \" ) if self . depth is not None and self . current_depth > self . depth : break self . current_depth += 1 new_data = self . _get_new_data () data . extend ( new_data ) return data","title":"Workers"},{"location":"code/workers/#workers","text":"","title":"Workers"},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker","text":"All data extractions goes here.","title":"DefaultWorker"},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker.__init__","text":"Parameters: Name Type Description Default factory BaseFactory see BaseFactory required Source code in extract_emails/workers/default_worker.py def __init__ ( self , factory : BaseFactory ): \"\"\" Args: factory: see `BaseFactory` \"\"\" self . website_url = factory . website_url self . browser = factory . browser self . depth = factory . depth self . max_links_from_page = factory . max_links_from_page self . link_filter = factory . link_filter self . data_extractors = factory . data_extractors self . links = [[ self . website_url ]] self . current_depth = 0","title":"__init__()"},{"location":"code/workers/#extract_emails.workers.default_worker.DefaultWorker.get_data","text":"Extract data from a given website Source code in extract_emails/workers/default_worker.py def get_data ( self ) -> List [ PageData ]: \"\"\"Extract data from a given website\"\"\" data : List [ PageData ] = [] while len ( self . links ): logger . debug ( f \"current_depth= { self . current_depth } \" ) if self . depth is not None and self . current_depth > self . depth : break self . current_depth += 1 new_data = self . _get_new_data () data . extend ( new_data ) return data","title":"get_data()"},{"location":"quick_start/intro/","text":"Intro \u00b6 There are several main parts in the framework: browser - Class to navigate through specific website and extract data from the webpages ( requests , selenium etc.) link filter - Class to extract URLs from a page corresponding to the website. There are two link filters: DefaultLinkFilter - Will extract all URLs corresponding to the website ContactInfoLinkFilter - Will extract only contact URLs, e.g. /contact/ , /about-us/ etc data extractor - Class to extract data from a page. At the moment there are two data extractors: EmailExtractor - Will extract all emails from the page LinkedinExtractor - Will extract all links to Linkedin profiles from the page factories - Combination of different link filters and data extractors , e.g. DefaultFilterAndEmailFactory or ContactFilterAndEmailAndLinkedinFactory DefaultWorker - All data extractions goes here Simple Usage: \u00b6 As library \u00b6 from pathlib import Path from extract_emails import DefaultFilterAndEmailFactory as Factory from extract_emails import DefaultWorker from extract_emails.browsers.requests_browser import RequestsBrowser as Browser from extract_emails.data_savers import CsvSaver websites = [ \"website1.com\" , \"website2.com\" , ] browser = Browser () data_saver = CsvSaver ( save_mode = \"a\" , output_path = Path ( \"output.csv\" )) for website in websites : factory = Factory ( website_url = website , browser = browser , depth = 5 , max_links_from_page = 1 ) worker = DefaultWorker ( factory ) data = worker . get_data () data_saver . save ( data ) As CLI tool \u00b6 $ extract-emails --help $ extract-emails --url https://en.wikipedia.org/wiki/Email -of output.csv -d 1 $ cat output.csv email,page,website bob@b.org,https://en.wikipedia.org/wiki/Email,https://en.wikipedia.org/wiki/Email","title":"Intro"},{"location":"quick_start/intro/#intro","text":"There are several main parts in the framework: browser - Class to navigate through specific website and extract data from the webpages ( requests , selenium etc.) link filter - Class to extract URLs from a page corresponding to the website. There are two link filters: DefaultLinkFilter - Will extract all URLs corresponding to the website ContactInfoLinkFilter - Will extract only contact URLs, e.g. /contact/ , /about-us/ etc data extractor - Class to extract data from a page. At the moment there are two data extractors: EmailExtractor - Will extract all emails from the page LinkedinExtractor - Will extract all links to Linkedin profiles from the page factories - Combination of different link filters and data extractors , e.g. DefaultFilterAndEmailFactory or ContactFilterAndEmailAndLinkedinFactory DefaultWorker - All data extractions goes here","title":"Intro"},{"location":"quick_start/intro/#simple-usage","text":"","title":"Simple Usage:"},{"location":"quick_start/intro/#as-library","text":"from pathlib import Path from extract_emails import DefaultFilterAndEmailFactory as Factory from extract_emails import DefaultWorker from extract_emails.browsers.requests_browser import RequestsBrowser as Browser from extract_emails.data_savers import CsvSaver websites = [ \"website1.com\" , \"website2.com\" , ] browser = Browser () data_saver = CsvSaver ( save_mode = \"a\" , output_path = Path ( \"output.csv\" )) for website in websites : factory = Factory ( website_url = website , browser = browser , depth = 5 , max_links_from_page = 1 ) worker = DefaultWorker ( factory ) data = worker . get_data () data_saver . save ( data )","title":"As library"},{"location":"quick_start/intro/#as-cli-tool","text":"$ extract-emails --help $ extract-emails --url https://en.wikipedia.org/wiki/Email -of output.csv -d 1 $ cat output.csv email,page,website bob@b.org,https://en.wikipedia.org/wiki/Email,https://en.wikipedia.org/wiki/Email","title":"As CLI tool"},{"location":"quick_start/logs/","text":"Logs \u00b6 There is loguru library under the hood. Settings \u00b6 import sys from loguru import logger logger . add ( sys . stderr , format = \" {time} {level} {message} \" , filter = \"my_module\" , level = \"INFO\" , ) Disable/Enable \u00b6 from loguru import logger logger . disable ( 'extract_emails' ) logger . enable ( 'extract_emails' )","title":"Logs"},{"location":"quick_start/logs/#logs","text":"There is loguru library under the hood.","title":"Logs"},{"location":"quick_start/logs/#settings","text":"import sys from loguru import logger logger . add ( sys . stderr , format = \" {time} {level} {message} \" , filter = \"my_module\" , level = \"INFO\" , )","title":"Settings"},{"location":"quick_start/logs/#disableenable","text":"from loguru import logger logger . disable ( 'extract_emails' ) logger . enable ( 'extract_emails' )","title":"Disable/Enable"},{"location":"quick_start/save_data/","text":"Save Data \u00b6 Data store as pydantic models Save as CSV \u00b6 from extract_emails import DefaultFilterAndEmailFactory as Factory from extract_emails.browsers.requests_browser import RequestsBrowser as Browser from extract_emails.models import PageData from extract_emails.workers import DefaultWorker browser = Browser () factory = Factory ( website_url = 'https://example.com' , browser = browser , depth = 0 , max_links_from_page = 0 ) extractor = DefaultWorker ( factory ) data = extractor . get_data () PageData . save_as_csv ( data , \"output.csv\" ) # cat output.csv website , page_url , email https : // example . com , https : // example . com / about - us , email @example . com https : // example . com , https : // example . com / about - us , email1 @example . com https : // example . com , https : // example . com / about - us , email2 @example . com https : // example . com , https : // example . com / about - us , email3 @example . com","title":"Save Data"},{"location":"quick_start/save_data/#save-data","text":"Data store as pydantic models","title":"Save Data"},{"location":"quick_start/save_data/#save-as-csv","text":"from extract_emails import DefaultFilterAndEmailFactory as Factory from extract_emails.browsers.requests_browser import RequestsBrowser as Browser from extract_emails.models import PageData from extract_emails.workers import DefaultWorker browser = Browser () factory = Factory ( website_url = 'https://example.com' , browser = browser , depth = 0 , max_links_from_page = 0 ) extractor = DefaultWorker ( factory ) data = extractor . get_data () PageData . save_as_csv ( data , \"output.csv\" ) # cat output.csv website , page_url , email https : // example . com , https : // example . com / about - us , email @example . com https : // example . com , https : // example . com / about - us , email1 @example . com https : // example . com , https : // example . com / about - us , email2 @example . com https : // example . com , https : // example . com / about - us , email3 @example . com","title":"Save as CSV"}]}